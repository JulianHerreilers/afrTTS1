{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from distance import levenshtein\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import import_ipynb\n",
    "import G2P_LSTM as g2p\n",
    "\n",
    "data_file = 'TTScorpora.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = g2p.cfg\n",
    "def dict_sorting (dict_file_name):\n",
    "    dict_file = open(dict_file_name, 'r')\n",
    "    lines_dict = dict_file.readlines()\n",
    "    dict_file.close()\n",
    "\n",
    "    graphemes = []\n",
    "    phonemes = []\n",
    "\n",
    "    for i in range(0, len(lines_dict)):\n",
    "        lines_dict[i] = lines_dict[i].split()\n",
    "        graphemes.append([*lines_dict[i][0]])\n",
    "        phonemes.append(lines_dict[i][1:])\n",
    "    phonemes = [a for b in phonemes for a in b]\n",
    "    graphemes = [a for b in graphemes for a in b]\n",
    "    graphemes = sorted(set(graphemes))\n",
    "    phonemes = sorted(set(phonemes))\n",
    "    return graphemes, phonemes\n",
    "\n",
    "dict_file_name = cfg.dict_name\n",
    "g_seq, p_seq = dict_sorting(dict_file_name)\n",
    "cfg.graphemes = [\"<pad>\", \"<unk>\", \"</s>\"] + g_seq\n",
    "cfg.phonemes = [\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"] + p_seq\n",
    "\n",
    "#Index to grapheme and phones for vectors\n",
    "cfg.graph2index = {g: idx for idx, g in enumerate(cfg.graphemes)}\n",
    "cfg.index2graph = {idx: g for idx, g in enumerate(cfg.graphemes)}\n",
    "\n",
    "cfg.phone2index = {p: idx for idx, p in enumerate(cfg.phonemes)}\n",
    "cfg.index2phone = {idx: p for idx, p in enumerate(cfg.phonemes)}\n",
    "\n",
    "# print(cfg.graphemes)\n",
    "# print(cfg.phonemes)\n",
    "cfg.g_vocab_size = len(cfg.graphemes)\n",
    "cfg.p_vocab_size = len(cfg.phonemes)\n",
    "# print(cfg.g_vocab_size, cfg.p_vocab_size)\n",
    "# print(cfg.phone2index)\n",
    "# print(cfg.index2phone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_nums_letters(seq):\n",
    "\n",
    "    def convert_num_word(num):\n",
    "        nums = ['0','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','30','40','50','60','70','80','90','100', '1000']\n",
    "        num_words = ['nul', 'een','twee', 'drie','vier','vyf', 'ses','sewe', 'agt', 'nege','tien','elf','twaalf','dertien','veertien','vyftien','sestien','sewentien', 'agtien', 'negentien', \n",
    "            'twintig','dertig', 'veertig', 'vyftig', 'sestig', 'sewentig', 'tagtig', 'negentig', 'honderd', 'duisend']\n",
    "        num_tens = ['10','11','12','13','14','15','16','17','18','19','20','30','40','50','60','70','80','90']\n",
    "        num_words_tens = ['tien','elf','twaalf','dertien','veertien','vyftien','sestien','sewentien', 'agtien', 'negentien', \n",
    "            'twintig','dertig', 'veertig', 'vyftig', 'sestig', 'sewentig', 'tagtig', 'negentig']\n",
    "        a = int(num)\n",
    "        ans = []\n",
    "        i=0\n",
    "        #print(a)\n",
    "        num_word = \"\"\n",
    "        tu_both = 0\n",
    "        t_added = 0\n",
    "        h_added = 0\n",
    "        tt_th_both = 0\n",
    "        tt_added = 0\n",
    "        \n",
    "        u =  str(int(a%10))\n",
    "        t =  str(int((a%100)/10))\n",
    "        h =  str(int((a%1000)/100))\n",
    "        tu = str(a%100)\n",
    "        th = str(int(a/1000)-int(a/10000)*10)\n",
    "        tt = str(int(a/10000))\n",
    "        tt_th = str(int(a/1000))\n",
    "\n",
    "        if tt_th:\n",
    "            for k in num_tens:\n",
    "                if tt_th == k and k!=\"0\":\n",
    "                    index = num_tens.index(k)\n",
    "                    num_word = num_word + num_words_tens[index]\n",
    "                    t_added = 1\n",
    "                    tt_th_both = 1\n",
    "                    break\n",
    "        if tt:\n",
    "            for k in nums:\n",
    "                if tt == k and k!=\"0\" and tt_th_both == 0:\n",
    "                    index = nums.index(k)\n",
    "                    num_word = num_word + num_words[index]\n",
    "                    tt_added = 1\n",
    "                    break\n",
    "        if th:\n",
    "            for k in nums:\n",
    "                if th == k and k!=\"0\" and tt_th_both == 0:\n",
    "                    index = nums.index(k)\n",
    "                    if tt_added: num_word = num_word +\" en \"+  num_words[index]\n",
    "                    else: num_word = num_word +  num_words[index]\n",
    "                    t_added =1\n",
    "                    break\n",
    "        \n",
    "        if t_added: num_word += \" duisend\"\n",
    "\n",
    "        if h:\n",
    "            for k in nums:\n",
    "                if h == k and k!=\"0\":\n",
    "                    index = nums.index(k)\n",
    "                    if t_added: num_word = num_word + \" en \"+ num_words[index] + ' honderd'\n",
    "                    else: num_word = num_word + num_words[index] + ' honderd'\n",
    "                    \n",
    "                    h_added = 1\n",
    "                    break\n",
    "        if tu:\n",
    "            for k in nums:\n",
    "                if tu == k and k!=\"0\":\n",
    "                    index = nums.index(k)\n",
    "                    if h_added: num_word = num_word +\" en \"+ num_words[index]\n",
    "                    else: num_word = num_word + num_words[index]\n",
    "                    tu_both = 1\n",
    "                    break\n",
    "        if t:\n",
    "            for k in nums:\n",
    "                if t == k and tu_both==0 and k!=\"0\":\n",
    "                    index = nums.index(k)\n",
    "                    if h_added: num_word = num_word +\" en \"+ num_words[index]\n",
    "                    else: num_word = num_word + num_words[index]\n",
    "                    break\n",
    "        if u:\n",
    "            for k in nums:\n",
    "                if u == k and tu_both==0 and k!=\"0\":\n",
    "                    index = nums.index(k)\n",
    "                    if h_added: num_word = num_word +\" en \"+ num_words[index]\n",
    "                    else: num_word = num_word + num_words[index]\n",
    "                    break\n",
    "            \n",
    "        return num_word\n",
    "\n",
    "\n",
    "    c = re.findall(r'[A-Za-z]+|\\d+', seq) #Source: https://stackoverflow.com/questions/28290492/python-splitting-numbers-and-letters-into-sub-strings-with-regular-expression\n",
    "    ans = []\n",
    "    for j,i in enumerate(c):\n",
    "        if i.isdigit():\n",
    "            ans.append(convert_num_word(i))\n",
    "        else: ans.append(i)\n",
    "\n",
    "    return ans\n",
    "\n",
    "stra=\"co.32despee208dy\"\n",
    "print(split_nums_letters(stra))\n",
    "print(split_nums_letters(\"80\"))\n",
    "print(split_nums_letters(\"80000\"))\n",
    "print(split_nums_letters('99999'))\n",
    "\n",
    "a = 28128\n",
    "print(int((a/1000)))\n",
    "print(int(a/10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data_file_name, nonums_data_file_name):\n",
    "\n",
    "    nums = ['0','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','30','40','50','60','70','80','90','100', '1000']\n",
    "    num_words = ['nul', 'een','twee', 'drie','vier','vyf', 'ses','sewe', 'agt', 'nege','tien','elf','twaalf','dertien','veertien','vyftien','sestien','sewentien', 'agtien', 'negentien', \n",
    "            'twintig','dertig', 'veertig', 'vyftig', 'sestig', 'sewentig', 'tagtig', 'negentig', 'honderd', 'duisend']\n",
    "\n",
    "    data_file = open(data_file_name, 'r')\n",
    "    lines_data = data_file.readlines()\n",
    "    data_file.close()\n",
    "\n",
    "\n",
    "    #Converting to lowercase\n",
    "    for i in range(0, len(lines_data)):\n",
    "        lines_data[i] = lines_data[i].split()\n",
    "        #l\n",
    "        \n",
    "        for j in range(0, len(lines_data[i])):\n",
    "            lines_data[i][j] = lines_data[i][j].lower()\n",
    "\n",
    "                 \n",
    "\n",
    "    with open(nonums_data_file_name, 'w') as file:\n",
    "        for i in range(0, len(lines_data)):\n",
    "            new_line = \" \".join([word for word in lines_data[i][1:]])\n",
    "            file.write('%s \\n' %new_line)\n",
    "    file.close()\n",
    "\n",
    "def see_all_unique_words(data_file_name, unique_words_file):\n",
    "\n",
    "    nums = ['0','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','30','40','50','60','70','80','90','100']\n",
    "    num_words = ['nul', 'een','twee', 'drie','vier','vyf', 'ses','sewe', 'agt', 'nege','tien','elf','twaalf','dertien','veertien','vyftien','sestien','sewentien', 'agtien', 'negentien', \n",
    "            'twintig','dertig', 'veertig', 'vyftig', 'sestig', 'sewentig', 'tagtig', 'negentig', 'een honderd']\n",
    "\n",
    "    data_file = open(data_file_name, 'r')\n",
    "    lines_data = data_file.readlines()\n",
    "    data_file.close()\n",
    "\n",
    "\n",
    "    #Converting to lowercase\n",
    "    for i in range(0, len(lines_data)):\n",
    "        lines_data[i] = split_nums_letters(lines_data[i])\n",
    "        for j in range(0, len(lines_data[i])):\n",
    "            lines_data[i][j] = lines_data[i][j].replace(\"-\", \" \")\n",
    "            lines_data[i][j] = lines_data[i][j].replace(\"+\", \" \")\n",
    "            lines_data[i][j] = lines_data[i][j].replace(\"%\", \"persent\")\n",
    "            lines_data[i][j] = lines_data[i][j].replace('\\\"', \"\")\n",
    "            lines_data[i][j] = lines_data[i][j].replace('\\'', \"\")\n",
    "            lines_data[i][j] = lines_data[i][j].replace(\"!\", \"\")\n",
    "            lines_data[i][j] = lines_data[i][j].replace(\"?\", \"\")\n",
    "            lines_data[i][j] = lines_data[i][j].replace(\".\", \"\")\n",
    "            lines_data[i][j] = lines_data[i][j].replace(\",\", \"\")\n",
    "            if j > 0: lines_data[i][j] = lines_data[i][j].replace(\"_\", \"\")\n",
    "            for k in nums:\n",
    "                if lines_data[i][j] == k:\n",
    "                    index = nums.index(k)\n",
    "                    lines_data[i][j] = num_words[index]\n",
    "                    break\n",
    "\n",
    "    lines_data_flattened = [i for j in lines_data for i in j]\n",
    "    # print(len(lines_data_flattened))               \n",
    "\n",
    "    unique = set(lines_data_flattened)\n",
    "    sorted_unique = sorted(unique)\n",
    "    # print(len(sorted_unique))\n",
    "    # print(sorted_unique) \n",
    "\n",
    "    with open(unique_words_file, 'w') as file:\n",
    "        for i in range(0, len(sorted_unique)):\n",
    "            new_line = sorted_unique[i]\n",
    "            file.write('%s\\n' %new_line)\n",
    "    file.close() \n",
    "\n",
    "data_file_name = \"TTScorpora.tsv\"\n",
    "data_file_without_index = \"no_index_file.txt\"\n",
    "data_unique_words = \"unique_words.txt\"\n",
    "preprocess_data(data_file_name, data_file_without_index)\n",
    "see_all_unique_words(data_file_without_index, data_unique_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pop_invalid_entries (data_file_name, dict_file_name, processed_data_file_name):\n",
    "    data_file = open(data_file_name, 'r')\n",
    "    dict_file = open(dict_file_name, 'r')\n",
    "    lines_data = data_file.readlines()\n",
    "    lines_dict = dict_file.readlines()\n",
    "    dict_file.close()\n",
    "    data_file.close()\n",
    "\n",
    "    dictionary = []\n",
    "    for i in range(0, len(lines_dict)):\n",
    "        lines_dict[i] = lines_dict[i].split()\n",
    "        dictionary.append(lines_dict[i][0])\n",
    "\n",
    "    words_to_add = []\n",
    "\n",
    "    for i in lines_data:\n",
    "        word_in_dict = 0\n",
    "        i = i.replace(\"\\n\",\"\")\n",
    "        for j in dictionary:\n",
    "            if i == j:\n",
    "                word_in_dict = 1\n",
    "                break\n",
    "        if word_in_dict == 0:\n",
    "            words_to_add.append(i)\n",
    "\n",
    "    # print(f\"words_to_add len:{len(words_to_add)}\")\n",
    "    # print(f\"words_to_add:{(words_to_add)}\")\n",
    "    with open(processed_data_file_name, 'w') as file:\n",
    "        for i in range(1, len(words_to_add)):\n",
    "            new_line = words_to_add[i]\n",
    "            file.write('%s\\n' %new_line)\n",
    "    file.close() \n",
    "\n",
    "data_file_name = \"TTScorpora.tsv\"\n",
    "dict_file_name = \"rcrl_apd.1.4.1.txt\"\n",
    "data_file_without_index = \"no_index_file.txt\"\n",
    "data_unique_words = \"unique_words.txt\"\n",
    "data_words_to_G2P = \"words_to_G2P.txt\"\n",
    "\n",
    "pop_invalid_entries (data_unique_words, dict_file_name, data_words_to_G2P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataLoading_forG2P(data_file):\n",
    "    def sortingWP (d):\n",
    "        w, p = [], []\n",
    "        for i in range(0, len(d)):\n",
    "\n",
    "            #w.append(d[i][0])\n",
    "            \n",
    "            #w.append(' '.join(j for j in d[i]))\n",
    "            w.append(\" \".join(d[i]))\n",
    "            p.append(\"<pad>\")\n",
    "        return w,p\n",
    "\n",
    "    with open(data_words_to_G2P) as f:\n",
    "        data_lines = f.readlines()\n",
    "    vocab_len = len(data_lines)\n",
    "    print(vocab_len)\n",
    "\n",
    "    #Potential to add 'n to train dataset\n",
    "    \n",
    "    for i in range(0, len(data_lines)):\n",
    "        data_lines[i] = data_lines[i].replace(\"\\n\",\"\")\n",
    "    train_data_lines = []\n",
    "    train_data_lines = data_lines\n",
    "    train_word, train_phonemes = sortingWP(data_lines)\n",
    "    \n",
    "    return train_word, train_phonemes\n",
    "\n",
    "data_words_to_G2P = \"words_to_G2P.txt\"\n",
    "data_set_G, data_set_P = DataLoading_forG2P(data_words_to_G2P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pd(model, iter, output_file, device):\n",
    "\n",
    "    \n",
    "    for i,batch in enumerate(iter):\n",
    "        print(device)\n",
    "        grapheme_vector, phoneme_vector, decoder_inputs, g_vec_len, p_vec_len = batch\n",
    "\n",
    "        grapheme_vector = grapheme_vector.to(device)\n",
    "        phoneme_vector = phoneme_vector.to(device)\n",
    "        decoder_inputs = phoneme_vector.to(device)\n",
    "        #print(\"1\")\n",
    "\n",
    "        phoneme_pred, phoneme_pred_sequence = model(grapheme_vector, g_vec_len,p_vec_len, decoder_inputs,phoneme_vector, False)\n",
    "\n",
    "        #print(\"2\")\n",
    "        graphemes, phonemes = [],[]\n",
    "        for j,k in zip(grapheme_vector,phoneme_pred_sequence):\n",
    "                # graphemes.append(g2p.data_decoder(phoneme_vector[j].cpu().numpy().tolist(), 1))\n",
    "                # phonemes.append(g2p.data_decoder(phoneme_pred_sequence[j].cpu().numpy().tolist(), 0))\n",
    "            graphemes.append(g2p.data_decoder(j,1))\n",
    "            phonemes.append(g2p.data_decoder(k,0))\n",
    "        pd = map(zip(graphemes, phonemes))\n",
    "        print(pd)\n",
    "\n",
    "    with open(output_file, 'a+') as file:\n",
    "        for i in range(0, len(pd)):\n",
    "            new_line = \" \".join([str(word) for word in [pd][i]])\n",
    "            file.write('%s \\n' %new_line)\n",
    "    file.close() \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder = g2p.Encoder(cfg.embed_dim, cfg.hidden_dim, cfg.g_vocab_size, cfg.n_layers, cfg.dropout)\n",
    "decoder = g2p.Decoder(cfg.embed_dim, cfg.hidden_dim, cfg.g_vocab_size, cfg.n_layers, cfg.dropout)\n",
    "\n",
    "model = g2p.G2PModel(encoder, decoder, cfg.device)\n",
    "model_params = \"best_models/G2pLSTM-e64h128n2.pt\"\n",
    "model.load_state_dict(torch.load(model_params))\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words_to_G2P = \"words_to_G2P.txt\"\n",
    "pd_file = \"pd_for_data.txt\"\n",
    "data_set_G, data_set_P = DataLoading_forG2P(data_words_to_G2P)\n",
    "dataset_to_g2p = g2p.G2PData(data_set_G, data_set_P)\n",
    "# print(len(dataset_to_g2p.graphemes))\n",
    "# print(len(dataset_to_g2p.phonemes))\n",
    "# print(dataset_to_g2p.graphemes)\n",
    "\n",
    "dataset_to_g2p_iter =  data.DataLoader(dataset_to_g2p,batch_size=cfg.batch_size, shuffle=True, collate_fn=g2p.padding_data)\n",
    "generate_pd(model, dataset_to_g2p_iter, pd_file, cfg.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_combiner(original_dict, extra_dict,ref_dict, new_dict):\n",
    "    dict1_file = open(original_dict, 'r')\n",
    "    dict2_file = open(extra_dict, 'r')\n",
    "    lines_dict1 = dict1_file.readlines()\n",
    "    lines_dict2 = dict2_file.readlines()\n",
    "    dict1_file.close()\n",
    "    dict2_file.close()\n",
    "    ref_file = open(ref_dict, 'r')\n",
    "    lines_dictref = ref_file.readlines()\n",
    "    ref_file.close()\n",
    "\n",
    "    for i in range(len(lines_dict1)):\n",
    "        lines_dict1[i] = lines_dict1[i].replace(\"\\n\",\"\")\n",
    "        lines_dict1[i] = lines_dict1[i].split()\n",
    "    for i in range(len(lines_dict2)):\n",
    "        lines_dict2[i] = lines_dict2[i].replace(\"\\n\",\"\")\n",
    "        lines_dict2[i] = lines_dict2[i].split()\n",
    "    for i in range(len(lines_dictref)):\n",
    "        lines_dictref[i] = lines_dictref[i].replace(\" \",\"\")\n",
    "        lines_dictref[i] = lines_dictref[i].replace(\"  \",\"\")\n",
    "        lines_dictref[i] = lines_dictref[i].replace(\"\\n\",\"\")\n",
    "    dict1_len = len(lines_dict1)\n",
    "    dict2_len = len(lines_dict2)\n",
    "    i,j = 0, 0\n",
    "    combined_dict = []\n",
    "    combined_dict.append(lines_dict1[0])\n",
    "    if lines_dictref[3608]< lines_dict1[3289][0]: print(\"True\")\n",
    "    # print(\".\"+lines_dictref[3608]+\".\")\n",
    "    # print(\".\"+lines_dict1[3289][0]+\".\")\n",
    "    # print(f\"lens - dict1:{dict1_len}, dict2:{dict2_len}\")\n",
    "    # print(lines_dictref)\n",
    "\n",
    "\n",
    "    for k in range(len(lines_dictref)):\n",
    "        \n",
    "        if lines_dictref[k] == lines_dict2[i][0]:\n",
    "            #print(\"equal\")\n",
    "            combined_dict.append(lines_dict2[i])\n",
    "            i+=1\n",
    "            print(\"Word added from dict 1\")\n",
    "        elif lines_dictref[k] == lines_dict1[j][0]:\n",
    "            #print(f\"word: {lines_dictref[k]} i: {i} j:{j} - {lines_dict2[i][0]} {lines_dict1[j][0]}\")\n",
    "            #print(f\"word: {len(lines_dictref[k])} i: {i} {len(lines_dict2[i][0])} \")\n",
    "            combined_dict.append(lines_dict1[j])\n",
    "            j+=1\n",
    "            print(\"Word added from dict 2\")\n",
    "        \n",
    "\n",
    "    print(\"Done\")\n",
    "    with open(new_dict, 'w') as file:\n",
    "        for l in range(0, len(combined_dict)):\n",
    "            new_line = \" \".join([str(word) for word in combined_dict[l]])\n",
    "            file.write('%s \\n' %new_line)\n",
    "    file.close() \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "original_dict_name = \"rcrl_apd.1.4.1.txt\"\n",
    "extra_dict_name = \"pd_for_data.txt\"\n",
    "new_dict_name = \"afr_za_dict\"\n",
    "ref_dict_name = \"testdict.txt\"\n",
    "dict_combiner(original_dict_name, extra_dict_name, ref_dict_name, new_dict_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_combiner(ordered_data_name, new_dict):\n",
    "    dict1_file = open(ordered_data_name, 'r', encoding='utf-16')\n",
    "    lines_dict1 = dict1_file.readlines()\n",
    "    dict1_file.close()\n",
    "\n",
    "    for i in range(len(lines_dict1)):\n",
    "        lines_dict1[i] = lines_dict1[i].split()\n",
    "\n",
    "    with open(new_dict, 'w') as file:\n",
    "        for l in range(0, len(lines_dict1)):\n",
    "            new_line = \" \".join([str(word) for word in lines_dict1[l]])\n",
    "            file.write('%s \\n' %new_line)\n",
    "    file.close() \n",
    "ordered_data_name = \"ordered_data.txt\"\n",
    "new_dict_name = \"afr_za_dict.za\"\n",
    "dict_combiner(ordered_data_name, new_dict_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
