{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from distance import levenshtein\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import import_ipynb\n",
    "\n",
    "data_file = 'TTScorpora.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    seed = '5'\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dict_name = 'rcrl_apd.1.4.1.txt'\n",
    "    epochs = 200\n",
    "    batch_size = 128\n",
    "    hidden_dim = 128\n",
    "    embed_dim = 128\n",
    "    dropout = 0.5\n",
    "    dec_max_len = 30\n",
    "    MAX_LENGTH = 20\n",
    "    teacher_forcing_ratio = 0.1\n",
    "    n_layers = 2\n",
    "    lr = 0.001\n",
    "\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_sorting (dict_file_name):\n",
    "    dict_file = open(dict_file_name, 'r')\n",
    "    lines_dict = dict_file.readlines()\n",
    "    dict_file.close()\n",
    "\n",
    "    graphemes = []\n",
    "    phonemes = []\n",
    "\n",
    "    for i in range(0, len(lines_dict)):\n",
    "        lines_dict[i] = lines_dict[i].split()\n",
    "        graphemes.append([*lines_dict[i][0]])\n",
    "        phonemes.append(lines_dict[i][1:])\n",
    "    phonemes = [a for b in phonemes for a in b]\n",
    "    graphemes = [a for b in graphemes for a in b]\n",
    "    graphemes = sorted(set(graphemes))\n",
    "    phonemes = sorted(set(phonemes))\n",
    "    return graphemes, phonemes\n",
    "\n",
    "dict_file_name = cfg.dict_name\n",
    "g_seq, p_seq = dict_sorting(dict_file_name)\n",
    "cfg.graphemes = [\"<pad>\", \"<unk>\", \"</s>\"] + g_seq\n",
    "cfg.phonemes = [\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"] + p_seq\n",
    "\n",
    "#Index to grapheme and phones for vectors\n",
    "cfg.graph2index = {g: idx for idx, g in enumerate(cfg.graphemes)}\n",
    "cfg.index2graph = {idx: g for idx, g in enumerate(cfg.graphemes)}\n",
    "\n",
    "cfg.phone2index = {p: idx for idx, p in enumerate(cfg.phonemes)}\n",
    "cfg.index2phone = {idx: p for idx, p in enumerate(cfg.phonemes)}\n",
    "\n",
    "# print(cfg.graphemes)\n",
    "# print(cfg.phonemes)\n",
    "cfg.g_vocab_size = len(cfg.graphemes)\n",
    "cfg.p_vocab_size = len(cfg.phonemes)\n",
    "# print(cfg.g_vocab_size, cfg.p_vocab_size)\n",
    "# print(cfg.phone2index)\n",
    "# print(cfg.index2phone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ses biljoen']\n",
      "['elf duisend een honderd en twintig']\n",
      "['tagtig duisend']\n",
      "['nege en negentig duisend nege honderd nege en negentig']\n",
      "['een honderd en tagtig duisend een honderd vyf en twintig']\n",
      "['een honderd en tagtig miljoen een honderd vyf en twintig duisend']\n",
      "['een honderd en nege biljoen nege honderd nege en negentig miljoen nege honderd nege en tagtig duisend en nege']\n",
      "['shje', 'hello', 'drie en twintig']\n"
     ]
    }
   ],
   "source": [
    "def split_nums_letters(seq):\n",
    "    nums = ['0','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','30','40','50','60','70','80','90','100']\n",
    "    num_words = ['nul', 'een','twee', 'drie','vier','vyf', 'ses','sewe', 'agt', 'nege','tien','elf','twaalf','dertien','veertien','vyftien','sestien','sewentien', 'agtien', 'negentien', \n",
    "        'twintig','dertig', 'veertig', 'vyftig', 'sestig', 'sewentig', 'tagtig', 'negentig', 'honderd']\n",
    "    \n",
    "    num_tens = ['10', '20','30','40','50','60','70','80','90']\n",
    "    num_words_tens = ['tien', 'twintig','dertig', 'veertig', 'vyftig', 'sestig', 'sewentig', 'tagtig', 'negentig']\n",
    "    nums_dict_combined = dict(zip(nums,num_words))\n",
    "    nums_tens_dict = dict(zip(num_tens, num_words_tens))\n",
    "\n",
    "    def num_to_word(h,t,u, flag, max_len, curr_str_len):\n",
    "        ans = \"\"\n",
    "        h_added = 0\n",
    "        t_u_combined = 0\n",
    "        req_suffix = 0\n",
    "        end = ['biljoen', 'miljoen', 'duisend']\n",
    "        #print(f\"h{h}, t{t}, u{u}\")\n",
    "        if h:\n",
    "            if h!=\"0\":\n",
    "                ans = ans + nums_dict_combined[h] + \" honderd\"\n",
    "                h_added = 1\n",
    "                req_suffix = 1\n",
    "        if h=='0' and (t!=\"0\" or u!=\"0\") and curr_str_len: \n",
    "            ans = ans + \"en \"\n",
    "\n",
    "        if (t+u):\n",
    "            if (t+u)!=\"00\" and nums_dict_combined.get(t+u):\n",
    "                    if h_added: ans = ans +\" en \"+ nums_dict_combined[t+u]\n",
    "                    else: ans = ans + nums_dict_combined[t+u]\n",
    "                    t_u_combined = 1\n",
    "                    req_suffix = 1\n",
    "\n",
    "        if t_u_combined == 0:\n",
    "            if t!=\"0\" and u!=\"0\" and t_u_combined == 0:\n",
    "                    if h_added: ans = ans +\" \"+ nums_dict_combined[u] + \" en \" +  num_words_tens[int(t)-1]\n",
    "                    else: ans = ans + nums_dict_combined[u] + \" en \" +  num_words_tens[int(t)-1]\n",
    "                    req_suffix = 1\n",
    "\n",
    "            elif t!=\"0\" and t_u_combined == 0:\n",
    "                    if h_added: ans = ans +\" en \"+ nums_tens_dict[int(t)]\n",
    "                    else: ans = ans + nums_dict_combined[t]\n",
    "                    req_suffix = 1\n",
    "\n",
    "            elif u!=\"0\" and t_u_combined == 0:\n",
    "                if h_added: ans = ans +\" en \"+ nums_dict_combined[u]\n",
    "                else: ans = ans + nums_dict_combined[u]\n",
    "                req_suffix = 1\n",
    "\n",
    "        if flag!=(max_len-1) and req_suffix and len(ans): ans = ans +\" \" + end[flag]\n",
    "\n",
    "        return ans\n",
    "\n",
    "\n",
    "\n",
    "    def convert_num_seq(num):\n",
    "\n",
    "        a = int(num)\n",
    "        max_len = 4\n",
    "\n",
    "\n",
    "        num_word_equiv = \"\"\n",
    "        tu_both = 0\n",
    "        t_added = 0\n",
    "        h_added = 0\n",
    "        tt_th_both = 0\n",
    "        tt_added = 0\n",
    "        ht_added = 0\n",
    "\n",
    "        bu = str(int(a/1000000000)-10*int(a/10000000000))\n",
    "        bt = str(int(a/10000000000)-10*int(a/100000000000))\n",
    "        bh = str(int(a/100000000000))\n",
    "        \n",
    "        mu = str(int(a/1000000)-10*int(a/10000000))\n",
    "        mt = str(int(a/10000000)-10*int(a/100000000))\n",
    "        mh = str(int((a%1000000000)/100000000))\n",
    "\n",
    "        uth = str(int(a/1000)-10*int(a/10000))\n",
    "        tth = str(int(a/10000)-10*int(a/100000))\n",
    "        hth = str(int((a%1000000)/100000))\n",
    "\n",
    "        u =  str(int(a%10))\n",
    "        t =  str(int((a%100)/10))\n",
    "        h =  str(int((a%1000)/100))\n",
    "        #t_u = str(a%100)\n",
    "\n",
    "        digits = [bh, bt, bu, mh, mt, mu, hth, tth, uth, h, t ,u]\n",
    "\n",
    "        for i in range(max_len):\n",
    "            flag = i #Use to add hondered or miljoen\n",
    "            new_str = num_to_word(digits[i*3], digits[i*3+1], digits[(i*3)+2], flag, max_len, len(num_word_equiv))\n",
    "            #if len(num_word_equiv) and len(new_str):num_word_equiv = num_word_equiv + \" en \" +  new_str\n",
    "            if len(new_str):\n",
    "                if len(num_word_equiv): num_word_equiv +=  (\" \" + new_str)\n",
    "                else: num_word_equiv +=  (new_str)\n",
    "\n",
    "            \n",
    "        return num_word_equiv\n",
    "\n",
    "    c = re.findall(r'[A-Za-z]+|\\d+', seq) #Source: https://stackoverflow.com/questions/28290492/python-splitting-numbers-and-letters-into-sub-strings-with-regular-expression\n",
    "    ans = []\n",
    "    for j,i in enumerate(c):\n",
    "        if i.isdigit():\n",
    "            ans.append(convert_num_seq(i))\n",
    "        else: ans.append(i)\n",
    "\n",
    "    return ans\n",
    "print(split_nums_letters(\"6000000000\"))\n",
    "print(split_nums_letters(\"11120\"))\n",
    "print(split_nums_letters(\"80000\"))\n",
    "print(split_nums_letters('99999'))\n",
    "print(split_nums_letters('180125'))\n",
    "print(split_nums_letters('180125000'))\n",
    "print(split_nums_letters('109999989009'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "a = [0,0,0]\n",
    "if a: print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "een\n"
     ]
    }
   ],
   "source": [
    "# a = 5\n",
    "# print(str(int(a%10)))\n",
    "# print(int((a%100)/10))\n",
    "# print(int((a%1000)/100))\n",
    "# a = [1,0,0]\n",
    "# if a: print(True)\n",
    "nums = ['0','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','30','40','50','60','70','80','90','100']\n",
    "num_words = ['nul', 'een','twee', 'drie','vier','vyf', 'ses','sewe', 'agt', 'nege','tien','elf','twaalf','dertien','veertien','vyftien','sestien','sewentien', 'agtien', 'negentien', \n",
    "    'twintig','dertig', 'veertig', 'vyftig', 'sestig', 'sewentig', 'tagtig', 'negentig', 'honderd']\n",
    "\n",
    "num_tens = ['10', '20','30','40','50','60','70','80','90']\n",
    "num_words_tens = ['tien', 'twintig','dertig', 'veertig', 'vyftig', 'sestig', 'sewentig', 'tagtig', 'negentig']\n",
    "nums_dict_combined = dict(zip(nums,num_words))\n",
    "nums_tens_dict = dict(zip(num_tens, num_words_tens))\n",
    "print(nums_dict_combined['1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data_file_name, nonums_data_file_name):\n",
    "\n",
    "    nums = ['0','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','30','40','50','60','70','80','90','100', '1000']\n",
    "    num_words = ['nul', 'een','twee', 'drie','vier','vyf', 'ses','sewe', 'agt', 'nege','tien','elf','twaalf','dertien','veertien','vyftien','sestien','sewentien', 'agtien', 'negentien', \n",
    "            'twintig','dertig', 'veertig', 'vyftig', 'sestig', 'sewentig', 'tagtig', 'negentig', 'honderd', 'duisend']\n",
    "\n",
    "    data_file = open(data_file_name, 'r')\n",
    "    lines_data = data_file.readlines()\n",
    "    data_file.close()\n",
    "\n",
    "\n",
    "    #Converting to lowercase\n",
    "    for i in range(0, len(lines_data)):\n",
    "        lines_data[i] = lines_data[i].split()\n",
    "        #l\n",
    "        \n",
    "        for j in range(0, len(lines_data[i])):\n",
    "            lines_data[i][j] = lines_data[i][j].lower()\n",
    "\n",
    "                 \n",
    "\n",
    "    with open(nonums_data_file_name, 'w') as file:\n",
    "        for i in range(0, len(lines_data)):\n",
    "            new_line = \" \".join([word for word in lines_data[i][1:]])\n",
    "            file.write('%s \\n' %new_line)\n",
    "    file.close()\n",
    "\n",
    "def see_all_unique_words(data_file_name, unique_words_file):\n",
    "\n",
    "    nums = ['0','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','30','40','50','60','70','80','90','100']\n",
    "    num_words = ['nul', 'een','twee', 'drie','vier','vyf', 'ses','sewe', 'agt', 'nege','tien','elf','twaalf','dertien','veertien','vyftien','sestien','sewentien', 'agtien', 'negentien', \n",
    "            'twintig','dertig', 'veertig', 'vyftig', 'sestig', 'sewentig', 'tagtig', 'negentig', 'een honderd']\n",
    "\n",
    "    data_file = open(data_file_name, 'r')\n",
    "    lines_data = data_file.readlines()\n",
    "    data_file.close()\n",
    "\n",
    "\n",
    "    #Converting to lowercase\n",
    "    for i in range(0, len(lines_data)):\n",
    "        #\" \".join([word for word in lines_data[i])\n",
    "        lines_data[i] = split_nums_letters(lines_data[i])\n",
    "        line = \" \".join(str(word) for word in lines_data[i])\n",
    "\n",
    "        lines_data[i] = line.split()\n",
    "        for j in range(0, len(lines_data[i])):\n",
    "            lines_data[i][j] = lines_data[i][j].replace(\"-\", \" \")\n",
    "            lines_data[i][j] = lines_data[i][j].replace(\"+\", \" \")\n",
    "            lines_data[i][j] = lines_data[i][j].replace(\"%\", \"persent\")\n",
    "            lines_data[i][j] = lines_data[i][j].replace('\\\"', \"\")\n",
    "            lines_data[i][j] = lines_data[i][j].replace('\\'', \"\")\n",
    "            lines_data[i][j] = lines_data[i][j].replace(\"!\", \"\")\n",
    "            lines_data[i][j] = lines_data[i][j].replace(\"?\", \"\")\n",
    "            lines_data[i][j] = lines_data[i][j].replace(\".\", \"\")\n",
    "            lines_data[i][j] = lines_data[i][j].replace(\",\", \"\")\n",
    "            if j > 0: lines_data[i][j] = lines_data[i][j].replace(\"_\", \"\")\n",
    "            for k in nums:\n",
    "                if lines_data[i][j] == k:\n",
    "                    index = nums.index(k)\n",
    "                    lines_data[i][j] = num_words[index]\n",
    "                    break\n",
    "\n",
    "    lines_data_flattened = [i for j in lines_data for i in j]\n",
    "    # print(len(lines_data_flattened))               \n",
    "\n",
    "    unique = set(lines_data_flattened)\n",
    "    sorted_unique = sorted(unique)\n",
    "    # print(len(sorted_unique))\n",
    "    # print(sorted_unique) \n",
    "\n",
    "    with open(unique_words_file, 'w') as file:\n",
    "        for i in range(0, len(sorted_unique)):\n",
    "            new_line = sorted_unique[i]\n",
    "            file.write('%s\\n' %new_line)\n",
    "    file.close() \n",
    "\n",
    "data_file_name = \"TTScorpora.tsv\"\n",
    "data_file_without_index = \"no_index_file.txt\"\n",
    "data_unique_words = \"unique_words.txt\"\n",
    "preprocess_data(data_file_name, data_file_without_index)\n",
    "see_all_unique_words(data_file_without_index, data_unique_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pop_invalid_entries (data_file_name, dict_file_name, processed_data_file_name):\n",
    "    data_file = open(data_file_name, 'r')\n",
    "    dict_file = open(dict_file_name, 'r')\n",
    "    lines_data = data_file.readlines()\n",
    "    lines_dict = dict_file.readlines()\n",
    "    dict_file.close()\n",
    "    data_file.close()\n",
    "\n",
    "    dictionary = []\n",
    "    for i in range(0, len(lines_dict)):\n",
    "        lines_dict[i] = lines_dict[i].split()\n",
    "        dictionary.append(lines_dict[i][0])\n",
    "\n",
    "    words_to_add = []\n",
    "\n",
    "    for i in lines_data:\n",
    "        word_in_dict = 0\n",
    "        i = i.replace(\"\\n\",\"\")\n",
    "        for j in dictionary:\n",
    "            if i == j:\n",
    "                word_in_dict = 1\n",
    "                break\n",
    "        if word_in_dict == 0:\n",
    "            words_to_add.append(i)\n",
    "\n",
    "    # print(f\"words_to_add len:{len(words_to_add)}\")\n",
    "    # print(f\"words_to_add:{(words_to_add)}\")\n",
    "    with open(processed_data_file_name, 'w') as file:\n",
    "        for i in range(0, len(words_to_add)):\n",
    "            new_line = words_to_add[i]\n",
    "            file.write('%s\\n' %new_line)\n",
    "    file.close() \n",
    "\n",
    "data_file_name = \"TTScorpora.tsv\"\n",
    "dict_file_name = \"rcrl_apd.1.4.1.txt\"\n",
    "data_file_without_index = \"no_index_file.txt\"\n",
    "data_unique_words = \"unique_words.txt\"\n",
    "data_words_to_G2P = \"words_to_G2P.txt\"\n",
    "\n",
    "pop_invalid_entries (data_unique_words, dict_file_name, data_words_to_G2P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataLoading_forG2P(data_file):\n",
    "    def sortingWP (d):\n",
    "        w, p = [], []\n",
    "        for i in range(0, len(d)):\n",
    "\n",
    "            #w.append(d[i][0])\n",
    "            \n",
    "            #w.append(' '.join(j for j in d[i]))\n",
    "            w.append(\" \".join(d[i]))\n",
    "            p.append(\"<pad>\")\n",
    "        return w,p\n",
    "\n",
    "    with open(data_words_to_G2P) as f:\n",
    "        data_lines = f.readlines()\n",
    "    vocab_len = len(data_lines)\n",
    "    print(vocab_len)\n",
    "\n",
    "    #Potential to add 'n to train dataset\n",
    "    \n",
    "    for i in range(0, len(data_lines)):\n",
    "        data_lines[i] = data_lines[i].replace(\"\\n\",\"\")\n",
    "    train_data_lines = []\n",
    "    train_data_lines = data_lines\n",
    "    train_word, train_phonemes = sortingWP(data_lines)\n",
    "    \n",
    "    return train_word, train_phonemes\n",
    "\n",
    "data_words_to_G2P = \"words_to_G2P.txt\"\n",
    "data_set_G, data_set_P = DataLoading_forG2P(data_words_to_G2P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pd(model, iter, output_file, device):\n",
    "\n",
    "    \n",
    "    for i,batch in enumerate(iter):\n",
    "        print(device)\n",
    "        grapheme_vector, phoneme_vector, decoder_inputs, g_vec_len, p_vec_len = batch\n",
    "\n",
    "        grapheme_vector = grapheme_vector.to(device)\n",
    "        phoneme_vector = phoneme_vector.to(device)\n",
    "        decoder_inputs = phoneme_vector.to(device)\n",
    "        #print(\"1\")\n",
    "\n",
    "        phoneme_pred, phoneme_pred_sequence = model(grapheme_vector, g_vec_len,p_vec_len, decoder_inputs,phoneme_vector, False)\n",
    "\n",
    "        #print(\"2\")\n",
    "        graphemes, phonemes = [],[]\n",
    "        for j,k in zip(grapheme_vector,phoneme_pred_sequence):\n",
    "                # graphemes.append(g2p.data_decoder(phoneme_vector[j].cpu().numpy().tolist(), 1))\n",
    "                # phonemes.append(g2p.data_decoder(phoneme_pred_sequence[j].cpu().numpy().tolist(), 0))\n",
    "            graphemes.append(g2p.data_decoder(j,1))\n",
    "            phonemes.append(g2p.data_decoder(k,0))\n",
    "        pd = map(zip(graphemes, phonemes))\n",
    "        print(pd)\n",
    "\n",
    "    with open(output_file, 'a+') as file:\n",
    "        for i in range(0, len(pd)):\n",
    "            new_line = \" \".join([str(word) for word in [pd][i]])\n",
    "            file.write('%s \\n' %new_line)\n",
    "    file.close() \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_combiner(original_dict, extra_dict,ref_dict, new_dict):\n",
    "    dict1_file = open(original_dict, 'r')\n",
    "    dict2_file = open(extra_dict, 'r')\n",
    "    lines_dict1 = dict1_file.readlines()\n",
    "    lines_dict2 = dict2_file.readlines()\n",
    "    dict1_file.close()\n",
    "    dict2_file.close()\n",
    "    ref_file = open(ref_dict, 'r')\n",
    "    lines_dictref = ref_file.readlines()\n",
    "    ref_file.close()\n",
    "\n",
    "    for i in range(len(lines_dict1)):\n",
    "        lines_dict1[i] = lines_dict1[i].replace(\"\\n\",\"\")\n",
    "        lines_dict1[i] = lines_dict1[i].split()\n",
    "    for i in range(len(lines_dict2)):\n",
    "        lines_dict2[i] = lines_dict2[i].replace(\"\\n\",\"\")\n",
    "        lines_dict2[i] = lines_dict2[i].split()\n",
    "    for i in range(len(lines_dictref)):\n",
    "        lines_dictref[i] = lines_dictref[i].replace(\" \",\"\")\n",
    "        lines_dictref[i] = lines_dictref[i].replace(\"  \",\"\")\n",
    "        lines_dictref[i] = lines_dictref[i].replace(\"\\n\",\"\")\n",
    "    dict1_len = len(lines_dict1)\n",
    "    dict2_len = len(lines_dict2)\n",
    "    i,j = 0, 0\n",
    "    combined_dict = []\n",
    "    combined_dict.append(lines_dict1[0])\n",
    "    if lines_dictref[3608]< lines_dict1[3289][0]: print(\"True\")\n",
    "    # print(\".\"+lines_dictref[3608]+\".\")\n",
    "    # print(\".\"+lines_dict1[3289][0]+\".\")\n",
    "    # print(f\"lens - dict1:{dict1_len}, dict2:{dict2_len}\")\n",
    "    # print(lines_dictref)\n",
    "\n",
    "\n",
    "    for k in range(len(lines_dictref)):\n",
    "        \n",
    "        if lines_dictref[k] == lines_dict2[i][0]:\n",
    "            #print(\"equal\")\n",
    "            combined_dict.append(lines_dict2[i])\n",
    "            i+=1\n",
    "            print(\"Word added from dict 1\")\n",
    "        elif lines_dictref[k] == lines_dict1[j][0]:\n",
    "            #print(f\"word: {lines_dictref[k]} i: {i} j:{j} - {lines_dict2[i][0]} {lines_dict1[j][0]}\")\n",
    "            #print(f\"word: {len(lines_dictref[k])} i: {i} {len(lines_dict2[i][0])} \")\n",
    "            combined_dict.append(lines_dict1[j])\n",
    "            j+=1\n",
    "            print(\"Word added from dict 2\")\n",
    "        \n",
    "\n",
    "    print(\"Done\")\n",
    "    with open(new_dict, 'w') as file:\n",
    "        for l in range(0, len(combined_dict)):\n",
    "            new_line = \" \".join([str(word) for word in combined_dict[l]])\n",
    "            file.write('%s \\n' %new_line)\n",
    "    file.close() \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "original_dict_name = \"rcrl_apd.1.4.1.txt\"\n",
    "extra_dict_name = \"pd_for_data.txt\"\n",
    "new_dict_name = \"afr_za_dict\"\n",
    "ref_dict_name = \"testdict.txt\"\n",
    "dict_combiner(original_dict_name, extra_dict_name, ref_dict_name, new_dict_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_combiner(ordered_data_name, new_dict):\n",
    "    dict1_file = open(ordered_data_name, 'r', encoding='utf-16')\n",
    "    lines_dict1 = dict1_file.readlines()\n",
    "    dict1_file.close()\n",
    "\n",
    "    for i in range(len(lines_dict1)):\n",
    "        lines_dict1[i] = lines_dict1[i].split()\n",
    "\n",
    "    with open(new_dict, 'w') as file:\n",
    "        for l in range(0, len(lines_dict1)):\n",
    "            new_line = \" \".join([str(word) for word in lines_dict1[l]])\n",
    "            file.write('%s \\n' %new_line)\n",
    "    file.close() \n",
    "ordered_data_name = \"ordered_data.txt\"\n",
    "new_dict_name = \"afr_za_dict.txt\"\n",
    "dict_combiner(ordered_data_name, new_dict_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', \"'\"])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g2p_start = {\"a\": \"A:\", \"b\": \"b\", \"c\":\"k\", \"d\":\"d\", \"e\":\"E\", \"f\":\"f\", \"g\":\"x\", \"h\":\"h_\", \"i\":\"i\",\n",
    "\"j\": \"j\", \"k\":\"k\", \"l\":\"l\", \"m\":\"m\", \"n\":\"n\", \"o\":\"O\", \"p\":\"p\", \"q\":\"k\", \"r\":\"r\", \"s\":\"s\",\n",
    "\"t\":\"t\", \"u\":\"9y\", \"v\":\"f\", \"w\":\"v\", \"x\":\"z\", \"y\":\"@i\", \"z\":\"z\", \"\\'\":\"A:\" }\n",
    "g2p_start.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
