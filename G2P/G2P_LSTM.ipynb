{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Initialization of Config Class/File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from distance import levenshtein\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "#from torchtext.data import BucketIterator\n",
    "\n",
    "\n",
    "seed = '4'\n",
    "if seed is not None:\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cfg class which is easliy translatible into a cfg file\n",
    "\n",
    "class Config:\n",
    "    seed = '5'\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dict_name = 'rcrl_apd.1.4.1.txt'\n",
    "    epochs = 200\n",
    "    batch_size = 128\n",
    "    hidden_dim = 128\n",
    "    embed_dim = 64\n",
    "    dropout = 0.5\n",
    "    dec_max_len = 30\n",
    "    MAX_LENGTH = 20\n",
    "    teacher_forcing_ratio = 0.5\n",
    "    n_layers = 2\n",
    "    lr = 0.001\n",
    "\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cfg.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of Datasets(using torchtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_sorting (dict_file_name):\n",
    "    dict_file = open(dict_file_name, 'r')\n",
    "    lines_dict = dict_file.readlines()\n",
    "    dict_file.close()\n",
    "\n",
    "    graphemes = []\n",
    "    phonemes = []\n",
    "\n",
    "    for i in range(0, len(lines_dict)):\n",
    "        lines_dict[i] = lines_dict[i].split()\n",
    "        graphemes.append([*lines_dict[i][0]])\n",
    "        phonemes.append(lines_dict[i][1:])\n",
    "    phonemes = [a for b in phonemes for a in b]\n",
    "    graphemes = [a for b in graphemes for a in b]\n",
    "    graphemes = sorted(set(graphemes))\n",
    "    phonemes = sorted(set(phonemes))\n",
    "    return graphemes, phonemes\n",
    "\n",
    "dict_file_name = cfg.dict_name\n",
    "g_seq, p_seq = dict_sorting(dict_file_name)\n",
    "cfg.graphemes = [\"<pad>\", \"<unk>\", \"</s>\"] + g_seq\n",
    "cfg.phonemes = [\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"] + p_seq\n",
    "\n",
    "#Index to grapheme and phones for vectors\n",
    "cfg.graph2index = {g: idx for idx, g in enumerate(cfg.graphemes)}\n",
    "cfg.index2graph = {idx: g for idx, g in enumerate(cfg.graphemes)}\n",
    "\n",
    "cfg.phone2index = {p: idx for idx, p in enumerate(cfg.phonemes)}\n",
    "cfg.index2phone = {idx: p for idx, p in enumerate(cfg.phonemes)}\n",
    "\n",
    "\n",
    "print(cfg.graphemes)\n",
    "print(cfg.phonemes)\n",
    "cfg.g_vocab_size = len(cfg.graphemes)\n",
    "cfg.p_vocab_size = len(cfg.phonemes)\n",
    "print(cfg.g_vocab_size, cfg.p_vocab_size)\n",
    "print(cfg.phone2index)\n",
    "print(cfg.index2phone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataLoading(dict_file_name, data_split):\n",
    "    def sortingWP (d):\n",
    "        w, p = [], []\n",
    "        for i in range(0, len(d)):\n",
    "            #w.append(d[i][0])\n",
    "            w.append(' '.join(d[i][0]))\n",
    "            p.append((' '.join(d[i][1:])))\n",
    "        return w,p\n",
    "\n",
    "    with open(dict_file_name) as f:\n",
    "        dict_lines = f.readlines()\n",
    "    vocab_len = len(dict_lines)\n",
    "    print(vocab_len)\n",
    "    random.shuffle(dict_lines)\n",
    "\n",
    "    #Potential to add 'n to train dataset\n",
    "    \n",
    "    for i in range(0, len(dict_lines)):\n",
    "        dict_lines[i] = dict_lines[i].split()\n",
    "    train_data_lines, test_data_lines, eval_data_lines = [], [], []\n",
    "    train_data_lines = dict_lines[0:int(data_split[0]*vocab_len)]\n",
    "    test_data_lines = dict_lines[int(data_split[0]*vocab_len):int((data_split[0]+data_split[1])*vocab_len)]\n",
    "    eval_data_lines = dict_lines[int((data_split[0]+data_split[1])*vocab_len):]\n",
    "\n",
    "    train_word, train_phonemes = sortingWP(train_data_lines)\n",
    "    test_word, test_phonemes = sortingWP(test_data_lines)\n",
    "    eval_word, eval_phonemes = sortingWP(eval_data_lines)\n",
    "    \n",
    "    return train_word, train_phonemes, test_word, test_phonemes, eval_word, eval_phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24175\n",
      "2417\n",
      "19340\n",
      "2418\n",
      "a n t i r e t r o v i r a l e\n",
      "a n t i r E t r u v i r A: l @\n"
     ]
    }
   ],
   "source": [
    "split = [0.8, 0.1, 0.1]\n",
    "train_word, train_phonemes, test_word, test_phonemes, eval_word, eval_phonemes = DataLoading(cfg.dict_name, split)\n",
    "\n",
    "#Sanity Check\n",
    "print(len(test_phonemes))\n",
    "print(len(train_phonemes))\n",
    "print(len(eval_phonemes))\n",
    "print(eval_word[0])\n",
    "print(eval_phonemes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Encoder & Decoder\n",
    "converts data to their dictionary equivalents based on indices(And decoder which will be used when finally checking sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_encoder(seq, isWord):\n",
    "    # Automatically encoders sequence with graph2index if words\n",
    "    tokenized_seq = []\n",
    "    if isWord: \n",
    "        seq = [*seq] + ['</s>']\n",
    "        seq = [i for i in seq if i!=\" \"]\n",
    "        for i in seq:\n",
    "            a = cfg.graph2index[i]\n",
    "            tokenized_seq.append(a)\n",
    "    #Else simply add end of sequence token to to phoneme sequences\n",
    "    else:\n",
    "        a = '<s> ' + str(seq) +' </s>'\n",
    "        seq = a.split(\" \")\n",
    "        ans = \"\"\n",
    "        for i in seq:\n",
    "            if i== 'o': i=\"O\"\n",
    "            elif i== 'h': i=\"h_\"\n",
    "            a = cfg.phone2index[i]\n",
    "            #ans = ans +\", \" + a\n",
    "            tokenized_seq.append(a)\n",
    "        #tokenized_seq = ans\n",
    "\n",
    "    #Tokenize sequence\n",
    "    return tokenized_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_word[0])\n",
    "print(train_phonemes[0])\n",
    "a = data_encoder(train_word[0], 1)\n",
    "b = data_encoder(train_phonemes[0], 0)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_decoder(sequence, isWord):\n",
    "    \"\"\"Converts index sequence back into corresponding letter tokens\"\"\"\n",
    "    if isWord: tokenizer = cfg.index2graph\n",
    "    else: tokenizer = cfg.index2phone\n",
    "    converted_sequence = []\n",
    "    \n",
    "    for i in sequence:\n",
    "        if tokenizer[i] == \"</s>\": break\n",
    "        a = tokenizer[i]\n",
    "        converted_sequence.append(a)\n",
    "    return converted_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [14, 4, 19, 23, 8, 12, 17, 2]\n",
    "b = [27, 17, 31, 34, 8, 30, 3]\n",
    "a = data_decoder(a, 1)\n",
    "b = data_decoder(b, 0)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Class \n",
    "(Adapted from https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G2PData (data.Dataset):\n",
    "    def __init__(self, graphemes, phonemes):\n",
    "        self.graphemes = graphemes\n",
    "        self.phonemes = phonemes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphemes)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        graphemes = self.graphemes[index]\n",
    "        phonemes = self.phonemes[index]\n",
    "\n",
    "        #Fetches encoded versions\n",
    "        grapheme_vector = data_encoder(graphemes, 1)\n",
    "        phoneme_vector = data_encoder(phonemes, 0)\n",
    "\n",
    "        #Omits </s> character\n",
    "        decoder_inputs = phoneme_vector[:-1]\n",
    "        phoneme_vector = phoneme_vector[1:]\n",
    "\n",
    "        #Used for padding purposes\n",
    "        g_vec_len = len(grapheme_vector) \n",
    "        p_vec_len = len(phoneme_vector)\n",
    "        \n",
    "        return grapheme_vector, phoneme_vector, decoder_inputs, g_vec_len, p_vec_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = G2PData(train_word, train_phonemes)\n",
    "testDataset = G2PData(test_word, test_phonemes)\n",
    "evalDataset = G2PData(eval_word, eval_phonemes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding: To ensure datasets in the same batch are of the same length (Could also use bucketiterator to choose strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Setup\n",
    "Based on seq2seq tutorial for Machine Translation (https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb)\n",
    "\n",
    "Potential implementation of Attention: https://www.kaggle.com/code/omershect/learning-pytorch-seq2seq-with-m5-data-set/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, g_vocab_size, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embed = embed_dim\n",
    "        self.hidden = hidden_dim\n",
    "        self.embed = nn.Embedding(g_vocab_size, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, n_layers, batch_first = True)\n",
    "    \n",
    "    def forward(self, graph_seq, graph_seq_len):\n",
    "        embed_inputs = self.embed(graph_seq)\n",
    "        inputs = self.dropout(embed_inputs)\n",
    "\n",
    "        #https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html\n",
    "        #packs padded sequences into tensor\n",
    "        input_tensor = pack_padded_sequence(inputs, graph_seq_len, batch_first=True, enforce_sorted=False)\n",
    "        output, (hidden, context) = self.lstm(input_tensor)\n",
    "\n",
    "        return hidden, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, p_vocab_size, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embed = embed_dim\n",
    "        self.hidden = hidden_dim\n",
    "        self.embed = nn.Embedding(p_vocab_size, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, n_layers, batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_dim ,p_vocab_size) #Predicts output\n",
    "    \n",
    "    def forward(self, decoder_inputs, hidden_init, context_init):\n",
    "\n",
    "        \n",
    "        embed_inputs = self.embed(decoder_inputs)\n",
    "        inputs = self.dropout(embed_inputs)\n",
    "\n",
    "        #is already a tensor\n",
    "\n",
    "        output, (hidden, context) = self.lstm(inputs, (hidden_init, context_init))\n",
    "\n",
    "\n",
    "        #Scaling output\n",
    "        activation_output = self.fc(output)\n",
    "        \n",
    "        \n",
    "        return activation_output, hidden,context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G2PModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.enc = encoder\n",
    "        self.dec = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, graph_seq, graph_seq_len,phone_seq_len, decoder_inputs, phoneme_target_vec = None, training = False, teacher_forcing = cfg.teacher_forcing_ratio):\n",
    "        \n",
    "        #Obtain hidden and context vectors from encoder\n",
    "        hidden_init, context_init = self.enc(graph_seq, graph_seq_len)\n",
    "        hidden, context = hidden_init, context_init\n",
    "\n",
    "        max_len = max(phone_seq_len)\n",
    "\n",
    "        phoneme_input_vec = decoder_inputs[:, :1]\n",
    "        outputs = [] \n",
    "        phone_pred_seq = []\n",
    "            \n",
    "        if training:\n",
    "            for i in range(0, max_len):\n",
    "\n",
    "                output, hidden, context = self.dec(phoneme_input_vec ,hidden, context)\n",
    "                outputs.append(output)\n",
    "                # phone_pred = torch.tensor(output.argmax(-1))\n",
    "\n",
    "                if random.random() > teacher_forcing: \n",
    "                    phoneme_input_vec = phoneme_target_vec[:,i]\n",
    "                    \n",
    "                else:  phoneme_input_vec = decoder_inputs[:,i]\n",
    "                phoneme_input_vec = torch.unsqueeze(phoneme_input_vec,1)\n",
    "\n",
    "        else: #for evaluation/prediction\n",
    "            for i in range(1, cfg.dec_max_len+1):\n",
    "                output, hidden, context = self.dec(phoneme_input_vec ,hidden, context)\n",
    "                \n",
    "                phone_pred = output.argmax(-1)\n",
    "                outputs.append(output)\n",
    "                phone_pred_seq.append(phone_pred)\n",
    "                phoneme_input_vec = phone_pred\n",
    "                #print(i)\n",
    "                #print(phoneme_input_vec.shape)\n",
    "            phone_pred_seq = torch.cat(phone_pred_seq, 1)\n",
    "            \n",
    "\n",
    "\n",
    "        output = torch.cat(outputs, 1)\n",
    "        \n",
    "        return output, phone_pred_seq\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def init_weights(m):\n",
    "#     for name, param in m.named_parameters():\n",
    "#         nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "# model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterators Initialization and Padding\n",
    "\n",
    "Padding_data takes a batch and pads it for every iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_data(batch):\n",
    "\n",
    "    #Each sequence has a form:\n",
    "    # grapheme_vector, phoneme_vector, decoder_inputs, g_vec_len, p_vec_len, graphemes, phonemes\n",
    "\n",
    "    def get_components(batch, index):\n",
    "        ans = []\n",
    "        for i in batch:\n",
    "            ans.append(i[index])\n",
    "        return ans\n",
    "    \n",
    "    def pad_seq(batch, index, max_len):\n",
    "        #ans = np.empty(cfg.batch_size)\n",
    "        ans = []\n",
    "        no_zeros_to_add = 0\n",
    "        for i in batch:\n",
    "            no_zeros_to_add = max_len - len(i[index])\n",
    "            ans.append(i[index] + [0] * no_zeros_to_add)\n",
    "        return torch.LongTensor(ans)\n",
    "    \n",
    "    grapheme_lens = [len(g[0]) for g in batch]\n",
    "\n",
    "    phonemes_lens = [len(p[1]) for p in batch]\n",
    "\n",
    "    input_maxlen = max(grapheme_lens)\n",
    "    output_maxlen = max(phonemes_lens)\n",
    "    padded_inputs = pad_seq(batch, 0, input_maxlen)\n",
    "    padded_outputs = pad_seq(batch, 1, output_maxlen)\n",
    "    padded_decoder_inputs = pad_seq(batch, 2, output_maxlen)\n",
    "\n",
    "    return padded_inputs, padded_outputs, padded_decoder_inputs, grapheme_lens, phonemes_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Loader Implementation\n",
    "#Shuffling not required as already loaded in a shuffled manner\n",
    "\n",
    "train_iter =  data.DataLoader(trainDataset,batch_size=cfg.batch_size, shuffle=True, collate_fn=padding_data)\n",
    "test_iter = data.DataLoader( testDataset,batch_size=cfg.batch_size, shuffle=False, collate_fn=padding_data)\n",
    "eval_iter = data.DataLoader(evalDataset,batch_size=cfg.batch_size, shuffle=False, collate_fn=padding_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model, Optimizer and Criterion Initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, device):\n",
    "    model.train() #sets model in training model\n",
    "    \n",
    "    loss_epoch = 0\n",
    "    \n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        grapheme_vector, phoneme_vector, decoder_inputs, g_vec_len, p_vec_len = batch\n",
    "\n",
    "        \n",
    "        #Placing vectors in GPU to streamline process\n",
    "        grapheme_vector = grapheme_vector.to(device)\n",
    "        phoneme_vector = phoneme_vector.to(device)\n",
    "        decoder_inputs = phoneme_vector.to(device)\n",
    "\n",
    "        optimizer.zero_grad() #Sets all gradients to zero\n",
    "\n",
    "\n",
    "        phoneme_pred,_ = model(grapheme_vector, g_vec_len,p_vec_len, decoder_inputs,phoneme_vector, True)\n",
    "\n",
    "        #phoneme_pred is in shape (batchsize, max_len, p_vocab_size) -> need to drop the last diameter\n",
    "\n",
    "        phoneme_pred = phoneme_pred.view(-1, phoneme_pred.shape[-1])\n",
    "        phoneme_vector = phoneme_vector.view(-1)\n",
    "\n",
    "        loss = criterion(phoneme_pred, phoneme_vector)\n",
    "\n",
    "        \n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        #Print loss every 50 batches\n",
    "        # if (i % 50 == 0) and (i != 0) and i <100 : print(f\" {i} batches completed: train loss: {loss}\")\n",
    "        # elif (i % 50 == 0) and (i != 0) : print(f\"{i} batches completed: train loss: {loss}\")\n",
    "\n",
    "\n",
    "        #Normalizing Loss per batch\n",
    "        loss_epoch += loss.item()/len(batch)\n",
    "\n",
    "\n",
    "    \n",
    "    return loss_epoch\n",
    "        \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WER_PER_calculation(phoneme_vector_text, phoneme_predict_text):\n",
    "    \"Word Error Rate - Words that are not 100% correct\"\n",
    "    \"PER - Rate at which phonemes are incorrectly generated - Levenshtein distance/n_phones in correct text\"\n",
    "    n_wrong_words =   0\n",
    "    total_phones_real = 0\n",
    "    n_incorrect_phones = 0\n",
    "\n",
    "    n_entries = len(phoneme_vector_text)\n",
    "\n",
    "    for i in range(n_entries):\n",
    "        total_phones_real += len(phoneme_vector_text[i])\n",
    "        ans = levenshtein(phoneme_vector_text[i], phoneme_predict_text[i])\n",
    "        n_incorrect_phones += ans\n",
    "        if ans !=0 : n_wrong_words +=1\n",
    "        \n",
    "    #phoneme_error_rate = incorrect_phonestotal_phones_real - incorrect as doesnt allow a sensible way to work out PER over entire epoch\n",
    "    #word_error_rate = n_wrong_words/n_entries\n",
    "    \n",
    "    return n_incorrect_phones, total_phones_real, n_wrong_words, n_entries\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, device):\n",
    "\n",
    "    def pad_p_vec(phoneme_vector, max_len):\n",
    "        phoneme_vector = phoneme_vector.cpu().numpy()\n",
    "        \n",
    "        ans = []\n",
    "\n",
    "        for i in phoneme_vector:\n",
    "            no_zeros_to_add = (max_len - len(i))\n",
    "            j = np.array(no_zeros_to_add * [0])\n",
    "            j = j.reshape((no_zeros_to_add, 1))\n",
    "            i = i.reshape((len(i), 1))\n",
    "\n",
    "            ans.append(np.concatenate((i, j)))\n",
    "            \n",
    "        return torch.LongTensor(ans)\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    loss_epoch = 0\n",
    "    n_incorrect_phones, total_phones_real, n_wrong_words, n_entries = 0, 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, batch in enumerate(iterator):\n",
    "            \n",
    "            grapheme_vector, phoneme_vector, decoder_inputs, g_vec_len, p_vec_len = batch\n",
    "\n",
    "            grapheme_vector = grapheme_vector.to(device)\n",
    "            phoneme_vector = phoneme_vector.to(device)\n",
    "            decoder_inputs = phoneme_vector.to(device)\n",
    "\n",
    "\n",
    "            #graph_seq, graph_seq_len, decoder_inputs, training = False, teacher_forcing = cfg.teacher_forcing_ratio\n",
    "\n",
    "            phoneme_pred, phoneme_pred_sequence = model(grapheme_vector, g_vec_len,p_vec_len, decoder_inputs,phoneme_vector, False) #False means only prediction and no training & teacher-forcing\n",
    "            #print(phoneme_pred.shape)\n",
    "            #print(f\"before:{phoneme_vector.shape}\")\n",
    "\n",
    "            phoneme_vector = pad_p_vec(phoneme_vector, cfg.dec_max_len)\n",
    "            phoneme_vector_copy = phoneme_vector\n",
    "\n",
    "            phoneme_vector_copy = phoneme_vector_copy.to(device)\n",
    "            phoneme_pred = phoneme_pred.view(-1, phoneme_pred.shape[-1])\n",
    "            phoneme_vector_copy = phoneme_vector_copy.view(-1)\n",
    "\n",
    "            loss = criterion(phoneme_pred, phoneme_vector_copy)\n",
    "            #print(f\"after:{phoneme_vector.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "            # First move tensor to CPU then to numpy array for decoding source: https://stackoverflow.com/questions/49768306/pytorch-tensor-to-numpy-array\n",
    "\n",
    "            # print(f\"batch: {i}\")\n",
    "            # # for j in range(5):\n",
    "            text_real, text_pred = [], []\n",
    "            #print(f\"real {phoneme_vector.shape}\")\n",
    "            #print(f\"pred {phoneme_pred_sequence.shape}\")\n",
    "            phoneme_vector = torch.squeeze(phoneme_vector)\n",
    "            for j in range(len(batch)):\n",
    "                text_real.append( data_decoder(phoneme_vector[j].cpu().numpy().tolist(), 0))\n",
    "                text_pred.append(data_decoder(phoneme_pred_sequence[j].cpu().numpy().tolist(), 0))\n",
    "    \n",
    "            a, b, c, d = WER_PER_calculation(text_pred, text_real)\n",
    "\n",
    "            #n_incorrect_phones, total_phones_real, n_wrong_words, n_entries\n",
    "\n",
    "            n_incorrect_phones += a\n",
    "            total_phones_real += b\n",
    "            n_wrong_words += c\n",
    "            n_entries += d\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "            loss_epoch += loss.item()/len(batch)\n",
    "  \n",
    "    word_error_rate = n_wrong_words / n_entries\n",
    "    phoneme_error_rate = n_incorrect_phones / total_phones_real\n",
    "\n",
    "    return loss_epoch, word_error_rate, phoneme_error_rate, n_wrong_words\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(cfg.embed_dim, cfg.hidden_dim, cfg.g_vocab_size, cfg.n_layers, cfg.dropout)\n",
    "decoder = Decoder(cfg.embed_dim, cfg.hidden_dim, cfg.g_vocab_size, cfg.n_layers, cfg.dropout)\n",
    "\n",
    "model = G2PModel(encoder, decoder, cfg.device)\n",
    "print(torch.cuda.is_available())\n",
    "model.to(device=cfg.device)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "optimizer = optim.Adam(model.parameters(), lr=cfg.lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0) #Ignores index corresponding to padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"models/G2pLSTM-e{cfg.embed_dim}h{cfg.hidden_dim}n{cfg.n_layers}.pt\"\n",
    "# torch.save(model.state_dict(), model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_losses = np.zeros([1, 6])\n",
    "print(combined_losses)\n",
    "combined_losses= np.vstack([combined_losses,(1,2,3,4,5,6)])\n",
    "print(combined_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_best_loss = math.inf\n",
    "#combined_losses = np.empty([cfg.epochs, 6])\n",
    "\n",
    "model_name = f\"models/G2pLSTM-e{cfg.embed_dim}h{cfg.hidden_dim}n{cfg.n_layers}.pt\"\n",
    "print(\"Training\")\n",
    "matrix_exists = 0\n",
    "\n",
    "\n",
    "for i in range(0, 60):\n",
    "    print(f\"Epoch {i}\")\n",
    "\n",
    "    train_loss = train(model, train_iter, optimizer, criterion, cfg.device)\n",
    "    \n",
    "    eval_loss, word_error_rate, phoneme_error_rate, n_wrong_words = evaluate(model, eval_iter, criterion, cfg.device)\n",
    "\n",
    "    if matrix_exists == 0:\n",
    "        combined_losses = np.array((i, train_loss, eval_loss, word_error_rate, phoneme_error_rate, n_wrong_words))\n",
    "        matrix_exists = 1\n",
    "    else:\n",
    "        combined_losses= np.vstack([combined_losses,(i, train_loss, eval_loss, word_error_rate, phoneme_error_rate, n_wrong_words)])\n",
    "\n",
    "    print(f\"train loss: {round(train_loss, 3)}    eval loss: {round(eval_loss, 3)}    WER: {round(word_error_rate, 3)}   PER: {round(phoneme_error_rate, 3)}    Incorrect words: {round(n_wrong_words, 3)}\")\n",
    "    if prev_best_loss > eval_loss:\n",
    "        prev_best_loss = eval_loss\n",
    "        best_epoch = i\n",
    "        if i > 10:\n",
    "            torch.save(model.state_dict(), model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# # from importlib import reload\n",
    "# # plt=reload(plt)\n",
    "plt.title('G2P LSTM Losses',fontsize=18)\n",
    "plt.xlabel('Epoch',fontsize=14)\n",
    "plt.ylabel('Loss',fontsize=14)\n",
    "\n",
    "plt.plot(combined_losses[:,0], combined_losses[:,1], label=\"Train Loss\")\n",
    "plt.plot(combined_losses[:,0], combined_losses[:,2], label=\"Eval Loss\")\n",
    "plt.locator_params(axis=\"x\", integer=True, tight=True)\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(combined_losses[:,0], combined_losses[:,3], label=\"wer\")\n",
    "plt.title('G2P LSTM WER',fontsize=18)\n",
    "plt.xlabel('Epoch',fontsize=14)\n",
    "plt.ylabel('WER',fontsize=14)\n",
    "plt.locator_params(axis=\"both\", integer=True, tight=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(combined_losses[:,0], combined_losses[:,4], label=\"PER\")\n",
    "plt.locator_params(axis=\"both\", integer=True, tight=True)\n",
    "plt.title('G2P PER',fontsize=18)\n",
    "plt.xlabel('Epoch',fontsize=14)\n",
    "plt.ylabel('PER',fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('G2P LSTM incorrect words',fontsize=18)\n",
    "plt.xlabel('Epoch',fontsize=14)\n",
    "plt.ylabel('number of incorrect words',fontsize=14)\n",
    "plt.plot(combined_losses[:,0], combined_losses[:,5], label=\"number of incorrect words\")\n",
    "plt.locator_params(axis=\"both\", integer=True, tight=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, eval_iter, criterion, cfg.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created with assistance from tutorials and adapted from Sources:\n",
    "1. https://github.com/Kyubyong/g2p - used for base notebook layout and initial introduction into structure of a G2P model - had no mention of teacherforcing\n",
    "2. https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e#58f2 - assistance in understanding various concepts and functions in [1]\n",
    "2. https://github.com/bentrevett/pytorch-seq2seq\n",
    "3. https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "\n",
    "4. Further assistance on DataLoader and BucketIterator from: https://colab.research.google.com/github/gmihaila/ml_things/blob/master/notebooks/pytorch/pytorchtext_bucketiterator.ipynb#scrollTo=ChQWVc4IUUPb\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
