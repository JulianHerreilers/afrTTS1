{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created with assistance from tutorials and adapted from Sources:\n",
    "1. https://github.com/Kyubyong/g2p - used for base notebook layout and initial introduction into structure of a G2P model\n",
    "2. https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e#58f2 - assistance in understanding various concepts and functions in [1]\n",
    "2. https://github.com/bentrevett/pytorch-seq2seq\n",
    "3. https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "\n",
    "4. Further assistance on DataLoader and BucketIterator from: https://colab.research.google.com/github/gmihaila/ml_things/blob/master/notebooks/pytorch/pytorchtext_bucketiterator.ipynb#scrollTo=ChQWVc4IUUPb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Initialization of Config Class/File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from distance import levenshtein\n",
    "import os\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "#from torchtext.data import BucketIterator\n",
    "\n",
    "\n",
    "seed = '4'\n",
    "if seed is not None:\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cfg class which is easliy translatible into a cfg file\n",
    "\n",
    "class Config:\n",
    "    seed = '5'\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dict_name = 'rcrl_apd.1.4.1.txt'\n",
    "    epochs = 35\n",
    "    batch_size = 128\n",
    "    hidden_dim = 128\n",
    "    embed_dim = 64\n",
    "    dropout = 0.5\n",
    "    dec_max_len = 30\n",
    "    MAX_LENGTH = 20\n",
    "    teacher_forcing_ratio = 0.5\n",
    "    n_layers = 2\n",
    "    lr = 0.001\n",
    "\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(cfg.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of Datasets(using torchtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<unk>', '</s>', \"'\", 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'á', 'ä', 'è', 'é', 'ê', 'ë', 'í', 'ï', 'ó', 'ô', 'ö', 'ú', 'û']\n",
      "['<pad>', '<unk>', '<s>', '</s>', '2:', '9', '9y', '@', '@i', '@u', 'A:', 'E', 'N', 'O', 'Of', 'S', 'Z', 'a', 'b', 'd', 'e', 'f', 'g', 'h_', 'i', 'i@', 'j', 'k', 'l', 'm', 'n', 'p', 'r', 's', 't', 'u', 'u@', 'v', 'w', 'x', 'y', 'z', '{']\n",
      "43 43\n",
      "{'<pad>': 0, '<unk>': 1, '<s>': 2, '</s>': 3, '2:': 4, '9': 5, '9y': 6, '@': 7, '@i': 8, '@u': 9, 'A:': 10, 'E': 11, 'N': 12, 'O': 13, 'Of': 14, 'S': 15, 'Z': 16, 'a': 17, 'b': 18, 'd': 19, 'e': 20, 'f': 21, 'g': 22, 'h_': 23, 'i': 24, 'i@': 25, 'j': 26, 'k': 27, 'l': 28, 'm': 29, 'n': 30, 'p': 31, 'r': 32, 's': 33, 't': 34, 'u': 35, 'u@': 36, 'v': 37, 'w': 38, 'x': 39, 'y': 40, 'z': 41, '{': 42}\n",
      "{0: '<pad>', 1: '<unk>', 2: '<s>', 3: '</s>', 4: '2:', 5: '9', 6: '9y', 7: '@', 8: '@i', 9: '@u', 10: 'A:', 11: 'E', 12: 'N', 13: 'O', 14: 'Of', 15: 'S', 16: 'Z', 17: 'a', 18: 'b', 19: 'd', 20: 'e', 21: 'f', 22: 'g', 23: 'h_', 24: 'i', 25: 'i@', 26: 'j', 27: 'k', 28: 'l', 29: 'm', 30: 'n', 31: 'p', 32: 'r', 33: 's', 34: 't', 35: 'u', 36: 'u@', 37: 'v', 38: 'w', 39: 'x', 40: 'y', 41: 'z', 42: '{'}\n"
     ]
    }
   ],
   "source": [
    "def dict_sorting (dict_file_name):\n",
    "    dict_file = open(dict_file_name, 'r')\n",
    "    lines_dict = dict_file.readlines()\n",
    "    dict_file.close()\n",
    "\n",
    "    graphemes = []\n",
    "    phonemes = []\n",
    "\n",
    "    for i in range(0, len(lines_dict)):\n",
    "        lines_dict[i] = lines_dict[i].split()\n",
    "        graphemes.append([*lines_dict[i][0]])\n",
    "        phonemes.append(lines_dict[i][1:])\n",
    "    phonemes = [a for b in phonemes for a in b]\n",
    "    graphemes = [a for b in graphemes for a in b]\n",
    "    graphemes = sorted(set(graphemes))\n",
    "    phonemes = sorted(set(phonemes))\n",
    "    return graphemes, phonemes\n",
    "\n",
    "dict_file_name = cfg.dict_name\n",
    "g_seq, p_seq = dict_sorting(dict_file_name)\n",
    "cfg.graphemes = [\"<pad>\", \"<unk>\", \"</s>\"] + g_seq\n",
    "cfg.phonemes = [\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"] + p_seq\n",
    "\n",
    "#Index to grapheme and phones for vectors\n",
    "cfg.graph2index = {g: idx for idx, g in enumerate(cfg.graphemes)}\n",
    "cfg.index2graph = {idx: g for idx, g in enumerate(cfg.graphemes)}\n",
    "\n",
    "cfg.phone2index = {p: idx for idx, p in enumerate(cfg.phonemes)}\n",
    "cfg.index2phone = {idx: p for idx, p in enumerate(cfg.phonemes)}\n",
    "\n",
    "\n",
    "print(cfg.graphemes)\n",
    "print(cfg.phonemes)\n",
    "cfg.g_vocab_size = len(cfg.graphemes)\n",
    "cfg.p_vocab_size = len(cfg.phonemes)\n",
    "print(cfg.g_vocab_size, cfg.p_vocab_size)\n",
    "print(cfg.phone2index)\n",
    "print(cfg.index2phone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataLoading(dict_file_name):\n",
    "    def sortingWP (d):\n",
    "        w, p = [], []\n",
    "        for i in range(0, len(d)):\n",
    "            #w.append(d[i][0])\n",
    "            w.append(' '.join(d[i][0]))\n",
    "            p.append((' '.join(d[i][1:])))\n",
    "        return w,p\n",
    "\n",
    "    with open(dict_file_name) as f:\n",
    "        dict_lines = f.readlines()\n",
    "    vocab_len = len(dict_lines)\n",
    "    print(vocab_len)\n",
    "    random.shuffle(dict_lines)\n",
    "\n",
    "    #Potential to add 'n to train dataset\n",
    "    \n",
    "    for i in range(0, len(dict_lines)):\n",
    "        dict_lines[i] = dict_lines[i].split()\n",
    "    train_data_lines, test_data_lines, eval_data_lines = [], [], []\n",
    "    train_data_lines = dict_lines[0:int(0.8*vocab_len)]\n",
    "    test_data_lines = dict_lines[int(0.8*vocab_len):int(0.9*vocab_len)]\n",
    "    eval_data_lines = dict_lines[int(0.9*vocab_len):]\n",
    "\n",
    "    train_word, train_phonemes = sortingWP(train_data_lines)\n",
    "    test_word, test_phonemes = sortingWP(test_data_lines)\n",
    "    eval_word, eval_phonemes = sortingWP(eval_data_lines)\n",
    "    \n",
    "    return train_word, train_phonemes, test_word, test_phonemes, eval_word, eval_phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24175\n",
      "2417\n",
      "19340\n",
      "2418\n",
      "p r i v a a t r e g\n",
      "p r i f A: t r { x\n"
     ]
    }
   ],
   "source": [
    "train_word, train_phonemes, test_word, test_phonemes, eval_word, eval_phonemes = DataLoading(cfg.dict_name)\n",
    "\n",
    "#Sanity Check\n",
    "print(len(test_phonemes))\n",
    "print(len(train_phonemes))\n",
    "print(len(eval_phonemes))\n",
    "print(eval_word[0])\n",
    "print(eval_phonemes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Encoder & Decoder\n",
    "converts data to their dictionary equivalents based on indices(And decoder which will be used when finally checking sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_encoder(seq, isWord):\n",
    "    # Automatically encoders sequence with graph2index if words\n",
    "    tokenized_seq = []\n",
    "    if isWord: \n",
    "        seq = [*seq] + ['</s>']\n",
    "        seq = [i for i in seq if i!=\" \"]\n",
    "        for i in seq:\n",
    "            a = cfg.graph2index[i]\n",
    "            tokenized_seq.append(a)\n",
    "    #Else simply add end of sequence token to to phoneme sequences\n",
    "    else:\n",
    "        a = '<s> ' + str(seq) +' </s>'\n",
    "        seq = a.split(\" \")\n",
    "        ans = \"\"\n",
    "        for i in seq:\n",
    "            if i== 'o': i=\"O\"\n",
    "            elif i== 'h': i=\"h_\"\n",
    "            a = cfg.phone2index[i]\n",
    "            #ans = ans +\", \" + a\n",
    "            tokenized_seq.append(a)\n",
    "        #tokenized_seq = ans\n",
    "\n",
    "    #Tokenize sequence\n",
    "    return tokenized_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o n t s n a p\n",
      "O n t s n a p\n",
      "[18, 17, 23, 22, 17, 4, 19, 2]\n",
      "[2, 13, 30, 34, 33, 30, 17, 31, 3]\n"
     ]
    }
   ],
   "source": [
    "print(train_word[0])\n",
    "print(train_phonemes[0])\n",
    "a = data_encoder(train_word[0], 1)\n",
    "b = data_encoder(train_phonemes[0], 0)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_decoder(sequence, isWord):\n",
    "    \"\"\"Converts index sequence back into corresponding letter tokens\"\"\"\n",
    "    if isWord: tokenizer = cfg.index2graph\n",
    "    else: tokenizer = cfg.index2phone\n",
    "    converted_sequence = []\n",
    "    for i in sequence:\n",
    "        if tokenizer[i] == \"</s>\": break\n",
    "        a = tokenizer[i]\n",
    "        converted_sequence.append(a)\n",
    "    return converted_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['k', 'a', 'p', 't', 'e', 'i', 'n']\n",
      "['k', 'a', 'p', 't', '@i', 'n']\n"
     ]
    }
   ],
   "source": [
    "a = [14, 4, 19, 23, 8, 12, 17, 2]\n",
    "b = [27, 17, 31, 34, 8, 30, 3]\n",
    "a = data_decoder(a, 1)\n",
    "b = data_decoder(b, 0)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Class \n",
    "(Adapted from https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G2PData (data.Dataset):\n",
    "    def __init__(self, graphemes, phonemes):\n",
    "        self.graphemes = graphemes\n",
    "        self.phonemes = phonemes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphemes)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        graphemes = self.graphemes[index]\n",
    "        phonemes = self.phonemes[index]\n",
    "\n",
    "        #Fetches encoded versions\n",
    "        grapheme_vector = data_encoder(graphemes, 1)\n",
    "        phoneme_vector = data_encoder(phonemes, 0)\n",
    "\n",
    "        #Omits </s> character\n",
    "        decoder_inputs = phoneme_vector[:-1]\n",
    "        phoneme_vector = phoneme_vector[1:]\n",
    "\n",
    "        #Used for padding purposes\n",
    "        g_vec_len = len(grapheme_vector) \n",
    "        p_vec_len = len(phoneme_vector)\n",
    "        \n",
    "        return grapheme_vector, phoneme_vector, decoder_inputs, g_vec_len, p_vec_len, graphemes, phonemes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = G2PData(train_word, train_phonemes)\n",
    "testDataset = G2PData(test_word, test_phonemes)\n",
    "evalDataset = G2PData(eval_word, eval_phonemes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding: To ensure datasets in the same batch are of the same length (Could also use bucketiterator to choose strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Initialization\n",
    "Based on seq2seq tutorial for Machine Translation (https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb)\n",
    "\n",
    "Potential implementation of Attention: https://www.kaggle.com/code/omershect/learning-pytorch-seq2seq-with-m5-data-set/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, g_vocab_size, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embed = embed_dim\n",
    "        self.hidden = hidden_dim\n",
    "        self.embed = nn.Embedding(g_vocab_size, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, n_layers, batch_first = True)\n",
    "    \n",
    "    def forward(self, graph_seq, graph_seq_len):\n",
    "        embed_inputs = self.embed(graph_seq)\n",
    "        inputs = self.dropout(embed_inputs)\n",
    "\n",
    "        #https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html\n",
    "        #packs padded sequences into tensor\n",
    "        input_tensor = pack_padded_sequence(inputs, graph_seq_len, batch_first=True, enforce_sorted=False)\n",
    "        output, (hidden, context) = self.lstm(input_tensor)\n",
    "\n",
    "        return hidden, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, p_vocab_size, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embed = embed_dim\n",
    "        self.hidden = hidden_dim\n",
    "        self.embed = nn.Embedding(p_vocab_size, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, n_layers, batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_dim ,p_vocab_size) #Predicts output\n",
    "    \n",
    "    def forward(self, decoder_inputs, hidden_init, context_init):\n",
    "\n",
    "        \n",
    "        embed_inputs = self.embed(decoder_inputs)\n",
    "        inputs = self.dropout(embed_inputs)\n",
    "\n",
    "        #is already a tensor\n",
    "\n",
    "        output, (hidden, context) = self.lstm(inputs, (hidden_init, context_init))\n",
    "\n",
    "\n",
    "        #Scaling output\n",
    "        activation_output = self.fc(output)\n",
    "        \n",
    "        \n",
    "        return activation_output, hidden,context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G2PModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.enc = encoder\n",
    "        self.dec = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, graph_seq, graph_seq_len,phone_seq_len, decoder_inputs, phoneme_target_vec = None, training = False, teacher_forcing = cfg.teacher_forcing_ratio):\n",
    "        \n",
    "        #Obtain hidden and context vectors from encoder\n",
    "        hidden_init, context_init = self.enc(graph_seq, graph_seq_len)\n",
    "        hidden, context = hidden_init, context_init\n",
    "\n",
    "        max_len = max(phone_seq_len)\n",
    "\n",
    "        phoneme_input_vec = decoder_inputs[:, :1]\n",
    "        outputs = [] \n",
    "        phone_pred_seq = []\n",
    "        if training:\n",
    "            for i in range(0, max_len):\n",
    "\n",
    "                output, hidden, context = self.dec(phoneme_input_vec ,hidden, context)\n",
    "                outputs.append(output)\n",
    "                #phone_pred = torch.tensor(output.argmax(-1))\n",
    "\n",
    "                if random.random() > teacher_forcing: \n",
    "                    phoneme_input_vec = phoneme_target_vec[:,i]\n",
    "                    \n",
    "                else:  phoneme_input_vec = decoder_inputs[:,i]\n",
    "                phoneme_input_vec = torch.unsqueeze(phoneme_input_vec,1)\n",
    "\n",
    "        else: #for evaluation/prediction\n",
    "            for i in range(1, cfg.dec_max_len):\n",
    "                output, hidden, context = self.dec(phoneme_input_vec ,hidden, context)\n",
    "                \n",
    "                phone_pred = output.argmax(-1)\n",
    "                outputs.append(output)\n",
    "                phone_pred_seq.append(phone_pred)\n",
    "                phoneme_input_vec = phone_pred\n",
    "                #print(i)\n",
    "                #print(phoneme_input_vec.shape)\n",
    "            phone_pred_seq = torch.cat(phone_pred_seq, 1)\n",
    "            \n",
    "\n",
    "\n",
    "        output = torch.cat(outputs, 1)\n",
    "        \n",
    "        return output, phone_pred_seq\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def init_weights(m):\n",
    "#     for name, param in m.named_parameters():\n",
    "#         nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "# model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterators Initialization and Padding\n",
    "\n",
    "Padding_data takes a batch and pads it for every iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_data(batch):\n",
    "\n",
    "    #Each sequence has a form:\n",
    "    # grapheme_vector, phoneme_vector, decoder_inputs, g_vec_len, p_vec_len, graphemes, phonemes\n",
    "\n",
    "    def get_components(batch, index):\n",
    "        ans = []\n",
    "        for i in batch:\n",
    "            ans.append(i[index])\n",
    "        return ans\n",
    "    \n",
    "    def pad_seq(batch, index, max_len):\n",
    "        #ans = np.empty(cfg.batch_size)\n",
    "        ans = []\n",
    "        no_zeros_to_add = 0\n",
    "        for i in batch:\n",
    "            no_zeros_to_add = max_len - len(i[index])\n",
    "            ans.append(i[index] + [0] * no_zeros_to_add)\n",
    "        return torch.LongTensor(ans)\n",
    "    \n",
    "    #input_lens = get_components(batch, 3)\n",
    "    grapheme_lens = [len(g[0]) for g in batch]\n",
    "\n",
    "    #output_lens = get_components(batch, 4)\n",
    "    phonemes_lens = [len(p[1]) for p in batch]\n",
    "\n",
    "    graphemes = get_components(batch, 5)\n",
    "    phonemes = get_components(batch, -1)\n",
    "\n",
    "    input_maxlen = max(grapheme_lens)\n",
    "    output_maxlen = max(phonemes_lens)\n",
    "    padded_inputs = pad_seq(batch, 0, input_maxlen)\n",
    "    padded_outputs = pad_seq(batch, 1, output_maxlen)\n",
    "    padded_decoder_inputs = pad_seq(batch, 2, output_maxlen)\n",
    "\n",
    "    return padded_inputs, padded_outputs, padded_decoder_inputs, grapheme_lens, phonemes_lens, graphemes, phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Loader Implementation\n",
    "#Shuffling not required as already loaded in a shuffled manner\n",
    "\n",
    "train_iter =  data.DataLoader(trainDataset,batch_size=cfg.batch_size, shuffle=True, collate_fn=padding_data)\n",
    "test_iter = data.DataLoader( testDataset,batch_size=cfg.batch_size, shuffle=False, collate_fn=padding_data)\n",
    "eval_iter = data.DataLoader(evalDataset,batch_size=cfg.batch_size, shuffle=False, collate_fn=padding_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "The model has 473,899 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(cfg.embed_dim, cfg.hidden_dim, cfg.g_vocab_size, cfg.n_layers, cfg.dropout)\n",
    "decoder = Decoder(cfg.embed_dim, cfg.hidden_dim, cfg.g_vocab_size, cfg.n_layers, cfg.dropout)\n",
    "\n",
    "model = G2PModel(encoder, decoder, cfg.device)\n",
    "print(torch.cuda.is_available())\n",
    "model.to(device=cfg.device)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer and Criterion Initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=cfg.lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0) #Ignores index corresponding to padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, device):\n",
    "    model.train() #sets model in training model\n",
    "    \n",
    "    loss_epoch = 0\n",
    "    #print(\"Training\")\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        grapheme_vector, phoneme_vector, decoder_inputs, g_vec_len, p_vec_len, graphemes, phonemes = batch\n",
    "\n",
    "        \n",
    "        #Placing vectors in GPU to streamline process\n",
    "        grapheme_vector = grapheme_vector.to(device)\n",
    "        phoneme_vector = phoneme_vector.to(device)\n",
    "        decoder_inputs = phoneme_vector.to(device)\n",
    "\n",
    "        optimizer.zero_grad() #Sets all gradients to zero\n",
    "\n",
    "        #graph_seq, graph_seq_len, decoder_inputs, training = False, teacher_forcing = cfg.teacher_forcing_ratio\n",
    "\n",
    "        phoneme_pred,_ = model(grapheme_vector, g_vec_len,p_vec_len, decoder_inputs,phoneme_vector, True)\n",
    "\n",
    "        #phoneme_pred is in shape (batchsize, N, p_vocab_size) -> need to drop the last diameter\n",
    "        # print(f\"real shape: {phoneme_vector.shape}\")\n",
    "        # print(f\"pred shape: {phoneme_pred.shape}\")\n",
    "        phoneme_pred = phoneme_pred.view(-1, phoneme_pred.shape[-1])\n",
    "        phoneme_vector = phoneme_vector.view(-1)\n",
    "\n",
    "        loss = criterion(phoneme_pred, phoneme_vector)\n",
    "\n",
    "        \n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        #Print loss every 50 batches\n",
    "        # if (i % 50 == 0) and (i != 0) and i <100 : print(f\" {i} batches completed: train loss: {loss}\")\n",
    "        # elif (i % 50 == 0) and (i != 0) : print(f\"{i} batches completed: train loss: {loss}\")\n",
    "\n",
    "        loss_epoch += loss.item()\n",
    "\n",
    "\n",
    "    \n",
    "    return loss_epoch\n",
    "        \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, device):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, batch in enumerate(iterator):\n",
    "            \n",
    "            grapheme_vector, phoneme_vector, decoder_inputs, g_vec_len, p_vec_len, graphemes, phonemes = batch\n",
    "\n",
    "            grapheme_vector = grapheme_vector.to(device)\n",
    "            phoneme_vector = phoneme_vector.to(device)\n",
    "            decoder_inputs = phoneme_vector.to(device)\n",
    "\n",
    "            optimizer.zero_grad() #Sets all gradients to zero\n",
    "\n",
    "            #graph_seq, graph_seq_len, decoder_inputs, training = False, teacher_forcing = cfg.teacher_forcing_ratio\n",
    "\n",
    "            phoneme_pred, phoneme_pred_sequence = model(grapheme_vector, g_vec_len,p_vec_len, decoder_inputs,phoneme_vector, False) #False means only prediction and no training & teacher-forcing\n",
    "            \n",
    "            # First move tensor to CPU then to numpy array for decoding source: https://stackoverflow.com/questions/49768306/pytorch-tensor-to-numpy-array\n",
    "\n",
    "            print(f\"batch: {i}\")\n",
    "            for j in range(5):\n",
    "                print(data_decoder(phoneme_vector[j].cpu().numpy(), 0))\n",
    "                print(data_decoder(phoneme_pred_sequence[j].cpu().numpy(), 0))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Phoneme Error Rate\n",
    "def determine_PER(true_target, pred_target):\n",
    "    total_phonemes, errors = 0, 0\n",
    "    for c_true, c_pred in zip(true_target, pred_target):\n",
    "        total_phonemes += len(c_true)\n",
    "        errors += levenshtein(c_true, c_pred)\n",
    "\n",
    "    PER = errors/total_phonemes\n",
    "    return PER, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "424.0776467323303\n",
      "Loss decreased\n",
      "Epoch 2\n",
      "293.98299288749695\n",
      "Loss decreased\n",
      "Epoch 3\n",
      "219.5517120361328\n",
      "Loss decreased\n",
      "Epoch 4\n",
      "173.93285381793976\n",
      "Loss decreased\n",
      "Epoch 5\n",
      "141.00932800769806\n",
      "Loss decreased\n",
      "Epoch 6\n",
      "116.83236026763916\n",
      "Loss decreased\n",
      "Epoch 7\n",
      "98.74001979827881\n",
      "Loss decreased\n",
      "Epoch 8\n",
      "84.99255195260048\n",
      "Loss decreased\n",
      "Epoch 9\n",
      "73.69068428874016\n",
      "Loss decreased\n",
      "Epoch 10\n",
      "65.2973039150238\n",
      "Loss decreased\n",
      "Epoch 11\n",
      "58.738361060619354\n",
      "Loss decreased\n",
      "Epoch 12\n",
      "52.65112681686878\n",
      "Loss decreased\n",
      "Epoch 13\n",
      "47.99060997366905\n",
      "Loss decreased\n",
      "Epoch 14\n",
      "44.106254264712334\n",
      "Loss decreased\n",
      "Epoch 15\n",
      "40.88694451749325\n",
      "Loss decreased\n",
      "Epoch 16\n",
      "37.782272189855576\n",
      "Loss decreased\n",
      "Epoch 17\n",
      "34.815278336405754\n",
      "Loss decreased\n",
      "Epoch 18\n",
      "32.69939823448658\n",
      "Loss decreased\n",
      "Epoch 19\n",
      "30.412136405706406\n",
      "Loss decreased\n",
      "Epoch 20\n",
      "28.270658925175667\n",
      "Loss decreased\n",
      "Epoch 21\n",
      "26.727284617722034\n",
      "Loss decreased\n",
      "Epoch 22\n",
      "24.834209382534027\n",
      "Loss decreased\n",
      "Epoch 23\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/junix/afrTTS/G2P/G2P_LSTM.ipynb Cell 34\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, cfg\u001b[39m.\u001b[39mepochs):\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     loss \u001b[39m=\u001b[39m train(model, train_iter, optimizer, criterion, cfg\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m#evaluate(model, eval_iter, cfg.device)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mprint\u001b[39m(loss)\n",
      "\u001b[1;32m/home/junix/afrTTS/G2P/G2P_LSTM.ipynb Cell 34\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad() \u001b[39m#Sets all gradients to zero\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m#graph_seq, graph_seq_len, decoder_inputs, training = False, teacher_forcing = cfg.teacher_forcing_ratio\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m phoneme_pred,_ \u001b[39m=\u001b[39m model(grapheme_vector, g_vec_len,p_vec_len, decoder_inputs,phoneme_vector, \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m#phoneme_pred is in shape (batchsize, N, p_vocab_size) -> need to drop the last diameter\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# print(f\"real shape: {phoneme_vector.shape}\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# print(f\"pred shape: {phoneme_pred.shape}\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m phoneme_pred \u001b[39m=\u001b[39m phoneme_pred\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, phoneme_pred\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "\u001b[1;32m/home/junix/afrTTS/G2P/G2P_LSTM.ipynb Cell 34\u001b[0m in \u001b[0;36mG2PModel.forward\u001b[0;34m(self, graph_seq, graph_seq_len, phone_seq_len, decoder_inputs, phoneme_target_vec, training, teacher_forcing)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, max_len):\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m         output, hidden, context \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdec(phoneme_input_vec ,hidden, context)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m         outputs\u001b[39m.\u001b[39mappend(output)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m         \u001b[39m#phone_pred = torch.tensor(output.argmax(-1))\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "\u001b[1;32m/home/junix/afrTTS/G2P/G2P_LSTM.ipynb Cell 34\u001b[0m in \u001b[0;36mDecoder.forward\u001b[0;34m(self, decoder_inputs, hidden_init, context_init)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(embed_inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m#is already a tensor\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m output, (hidden, context) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(inputs, (hidden_init, context_init))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m#Scaling output\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m activation_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(output)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:581\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    580\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 581\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    582\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    583\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    584\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    585\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prev_best_loss = math.inf\n",
    "for i in range(0, cfg.epochs):\n",
    "    print(f\"Epoch {i+1}\")\n",
    "    loss = train(model, train_iter, optimizer, criterion, cfg.device)\n",
    "    #evaluate(model, eval_iter, cfg.device)\n",
    "    print(loss)\n",
    "    if prev_best_loss > loss:\n",
    "        prev_best_loss = loss\n",
    "        print(\"Loss decreased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 0\n",
      "['p', 'r', 'i', 'f', 'A:', 't', 'r', '{', 'x']\n",
      "['p', 'r', 'i', 'f', 'A:', 'r', 't', 'x', '@']\n",
      "['r', '{', 'x', 's', 'A:', 'k', '@']\n",
      "['r', '{', 'x', 's', 'A:', 'k', '@']\n",
      "['s', 'a', 'f', 'A:', 'r', 'i']\n",
      "['s', 'a', 'f', 'A:', 'r', '@']\n",
      "['a', 'r', 'p', 'E', 'd', 'Z', 'i', 'u']\n",
      "['a', 'r', 'p', '@', 'x', '@', 'x']\n",
      "['l', '@', 'N', 'k', '@', 'r', 'k', 'a', 'n', 't']\n",
      "['l', '@', 'N', 'k', '@', 'r', 'k', 'a', 'n', 't']\n",
      "batch: 1\n",
      "['f', '{', 'r', 'd', '@', 'r']\n",
      "['f', '@', 'r', 'd', '@', 'r']\n",
      "['v', '{', 'x', 'k', 'O', 'm']\n",
      "['v', '{', 'x', 'k', 'O', 'm']\n",
      "['{', 'x']\n",
      "['{', 'x']\n",
      "['v', '{', 'r', 'k', 'i']\n",
      "['v', '{', 'r', 'k', 'i']\n",
      "['i@', 'v', '@']\n",
      "['i@', 'v', '@']\n",
      "batch: 2\n",
      "['d', 'r', 'u@', 'm']\n",
      "['d', 'r', 'u@', 'm']\n",
      "['b', '@', 'k', 'l', '@', 'N', 'k']\n",
      "['b', '@', 'k', 'l', '@', 'N', 'k']\n",
      "['s', 'a', 'x', '@', 'A:', 'r', 'd', '@']\n",
      "['s', 'a', 'x', '@', 'A:', 'r', 'd', '@']\n",
      "['v', '9', 'r', 'x', 'r', 'i@', 'p']\n",
      "['v', '9', 'r', 'x', 'r', 'i@', 'p']\n",
      "['d', '{', 'r', 'd', '@']\n",
      "['d', '{', 'r', 'd', '@']\n",
      "batch: 3\n",
      "['s', 'l', '{', 'x', 't', '@', 'r']\n",
      "['s', 'l', '{', 'x', 't', '@', 'r']\n",
      "['x', 'i', 'u', 'x', 'r', 'a', 'f', 'i']\n",
      "['x', '@', 'u', 'x', 'r', 'a', 'f', 'i']\n",
      "['t', '{', 'x', 'n', 'i', 'k', 'O', 'n', 's']\n",
      "['t', 'E', 'x', 'n', '@', 'k', 'O', 'n', 's']\n",
      "['A:', 'n', 'm', '{', 'r', 'k', 'l', '@', 'k']\n",
      "['A:', 'n', 'm', '{', 'r', 'k', 'l', '@', 'k']\n",
      "['t', 'u', 'x', '@', 'n', 'i@', 'n', 't', 'h_', '@i', 't']\n",
      "['t', 'u', 'x', '@', 'n', 'E', 'n', 't', 'h_', '@i', 't']\n",
      "batch: 4\n",
      "['A:', 'n', 't', 'r', '{', 'k', 'l', '@', 'k', '@', 'r']\n",
      "['A:', 'n', 't', 'r', '{', 'k', 'l', '@', 'k', '@', 'r']\n",
      "['d', '@', 's', 't', 'r', '@', 'k', 's', 'r', 'A:', 't']\n",
      "['d', '@', 's', 't', 'r', '@', 'k', 's', 'r', 'A:', 't']\n",
      "['t', 'r', '@i', 't', '@', 'r']\n",
      "['t', 'r', '@i', 't', '@', 'r']\n",
      "['s', 'k', 'A:', 'k', '@', 'l', 'v', '{', 'r', 'k']\n",
      "['s', 'k', 'A:', 'k', '@', 'l', 'v', '{', 'r', 'k']\n",
      "['v', 'i@', 'k']\n",
      "['v', 'i@', 'k']\n",
      "batch: 5\n",
      "['b', '@', 's', 'm', 'E', 't', '@', 'N']\n",
      "['b', '@', 's', 'm', 'E', 't', '@', 'N']\n",
      "['x', 'r', 'E', 'n', 's', 'd', 'r', 'A:', 't']\n",
      "['x', 'r', 'E', 'n', 's', 'd', 'r', 'A:', 't']\n",
      "['f', 'a', 's', 'f', 'r', 'A:']\n",
      "['f', 'a', 's', 'f', 'r', 'A:']\n",
      "['r', '@', 'f', 'i', 'r']\n",
      "['r', '@', 'f', 'i', 'r']\n",
      "['v', 'E', 'b', 'v', '{', 'r', 'f']\n",
      "['v', 'E', 'b', 'v', '{', 'r', 'f']\n",
      "batch: 6\n",
      "['v', 'u', 'd', '@', 'n', 't']\n",
      "['v', 'u', 'd', '@', 'n', 't']\n",
      "['b', 'l', 'A:', 's', 'i', 's']\n",
      "['b', 'l', 'A:', 's', 'i', 's']\n",
      "['s', 't', 'r', '9', 'k', 't', 'S', 'y', 'r', 'i@', 'l']\n",
      "['s', 't', 'r', '9', 'k', 't', 'y', 'r', 'i@', 'l']\n",
      "['b', '@', 'n', '@', 'x', '@', 's', 't', 'a', 'p']\n",
      "['b', '@', 'n', '@', 'x', '@', 's', 't', 'a', 'p']\n",
      "['9y', 't', 'x', '@', 'x', 'i@']\n",
      "['9y', 't', 'x', '@', 'x', 'i@']\n",
      "batch: 7\n",
      "['p', '@i', 'n', 's']\n",
      "['p', '@i', 'n', 's']\n",
      "['f', '@', 'r', 'i@', 'n', '@', 'x', '@', 'n', 'd', '@']\n",
      "['f', '@', 'r', 'i@', 'n', '@', 'x', '@', 'n', 'd', '@']\n",
      "['l', 'E', 's', '@']\n",
      "['l', 'E', 's', '@']\n",
      "['x', 'r', 'u', 'p', 'l', 'i@', 'd', '@']\n",
      "['x', 'r', 'u', 'p', 'l', 'i@', 'd', '@']\n",
      "['f', '@', 'r', 'k', 'l', 'A:', 'r', '@', 'N', 's']\n",
      "['f', '@', 'r', 'k', 'l', 'A:', 'r', '@', 'N', 's']\n",
      "batch: 8\n",
      "['h_', 'y', 'm', '2:', 'r']\n",
      "['h_', 'y', 'm', '2:', 'r']\n",
      "['u@', 'r', 's', 't', 'i@', 'k']\n",
      "['u@', 'r', 's', 't', 'i@', 'k']\n",
      "['l', 'i@', 'r', 'i@', 'n', 'h_', '@i', 't', 'p', 'l', 'a', 'n']\n",
      "['l', 'i@', 'r', 'i@', 'n', 'h_', '@i', 't', 's', 'p', 'a', 'l']\n",
      "['a', 'b', 'd', 'O', 'm', 'i', 'n', 'A:', 'l', '@']\n",
      "['a', 't', 'm', 'O', 'n', 'i', 'A:', 'l', '@']\n",
      "['t', 'E', 'm', 'p', 'u', 'r', 'i@', 'l', '@']\n",
      "['t', 'E', 'm', 'p', 'u', 'r', 'i@', 'l', '@']\n",
      "batch: 9\n",
      "['x', '@u', 't', 'm', 'a', 'r', 'k']\n",
      "['x', '@u', 't', 'm', 'a', 'r', 'k']\n",
      "['9y', 't', 'f', 'r', 'A:']\n",
      "['9y', 't', 'f', 'r', 'A:']\n",
      "['m', 'a', 'x', 's', 'm', '@', 's', 'b', 'r', '9y', 'k']\n",
      "['m', 'a', 'x', 's', 'm', '@', 'x', 'r', '9y', 's']\n",
      "['h_', 'a', 'l', 'f', 'x', '@', 'b', 'i', 't']\n",
      "['h_', 'a', 'l', 'f', 'x', '@', 'b', 'i', 't']\n",
      "['h_', '9y', 's', 'b', '@', 'd', 'i', 'n', 'd', '@']\n",
      "['h_', '9y', 's', 'b', '@', 'd', 'i', 'n', 'd', '@']\n",
      "batch: 10\n",
      "['t', 'w', '@', 'n', 't', '@', 'x', 's', 't', '@']\n",
      "['t', 'w', '@', 'n', 't', '@', 'N', 's', '@']\n",
      "['r', '{', 'x', 't', '@', 'r', 'f', 'l', '2:', '@', 'l']\n",
      "['r', '{', 'x', 't', '@', 'r', 'f', 'l', '2:', 'l']\n",
      "['f', '@', 'r', 'v', 'A:', 'r', 'l', 'u@', 's', 'd', '@']\n",
      "['f', '@', 'r', 'v', 'A:', 'r', 'l', 'u@', 's', 'd', '@']\n",
      "['f', '@', 'r', 'd', '9', 'b', '@', 'l']\n",
      "['f', '@', 'r', 't', '9', 'b', '@', 'l']\n",
      "['f', 'i', 'd', '@', 's']\n",
      "['f', 'i', 'd', '@', 's']\n",
      "batch: 11\n",
      "['x', 'a', 'N', '@']\n",
      "['x', 'a', 'N', '@']\n",
      "['b', '@', 'u@', 'x', 'd', '@']\n",
      "['b', '@', 'u@', 'x', 'd', '@']\n",
      "['f', 'O', 'l', 'k', 'i']\n",
      "['f', 'O', 'l', 'k', 'i']\n",
      "['s', 'p', 'O', 'r', 't', 'm', 'a', 'n']\n",
      "['s', 'p', 'O', 'r', 't', 'm', 'a', 'n']\n",
      "['f', 'a', 'k', 's', 'p', 'E', 's', '@', 'f', 'i', 'k', '@']\n",
      "['f', 'a', 'k', 's', 'p', 'E', 's', 'i', 'k', 'i', 'f', '@']\n",
      "batch: 12\n",
      "['@', 'n', 't', '@', 'r', 'a', 'f', 'h_', 'a', 'N', 'k', 'l', '@', 'k', '@']\n",
      "['@', 'n', 't', '@', 'r', 'a', 'f', 'd', '@', 'x', 'l', 'a', 'k', '@']\n",
      "['a', 'x', 't', '@', 'r', 'a', 'f']\n",
      "['a', 'x', 't', '@', 'r', 'a', 'f']\n",
      "['s', 'k', '9y', 'l', 'n', 'A:', 'm', '@']\n",
      "['s', 'k', '9y', 'l', 'n', 'A:', 'm', '@']\n",
      "['p', 'l', '{', 'k', 'l', 'u@', 't', '@', 'N']\n",
      "['p', 'l', 'E', 'k', 'O', 'l', 't', '@', 'N']\n",
      "['p', 'r', '@i', 's', 'x', '@', 'x', 'i@']\n",
      "['p', 'r', '@i', 's', 'x', '@', 'x', 'i@']\n",
      "batch: 13\n",
      "['a', 'n', 'E', 'k', 's', 'i@', 'r']\n",
      "['a', 'n', 'i@', 'k', 's', 'i@', 'r']\n",
      "['x', '@', 'd', 'y', 'r']\n",
      "['x', '@', 'd', 'y', 'r']\n",
      "['s', 'k', 'A:', 'r']\n",
      "['s', 'k', 'A:', 'r']\n",
      "['s', '9', 'b', 't', 'i', 'l', '@']\n",
      "['s', '9', 'b', 't', 'i', 'l', '@']\n",
      "['@', 'm', 'p', 'l', '@', 's', 'i', 't']\n",
      "['@', 'm', 'p', 'l', 'i', 's', 'i', 't']\n",
      "batch: 14\n",
      "['f', 'O', 'l', 'p', '9', 'n', 't', '@']\n",
      "['f', 'O', 'l', 'p', '9', 'n', 't', '@']\n",
      "['i', 't', 's']\n",
      "['i', 't', 's']\n",
      "['t', '9y', 's', 'f', '{', 'l', 't']\n",
      "['t', '9y', 's', 'f', '{', 'l', 't']\n",
      "['t', 'O', 'p', 'b', '@', 's', 't', 'y', 'r']\n",
      "['t', 'O', 'p', 'b', '@', 's', 't', 'y', 'r']\n",
      "['l', 'a', 'N', 's', 'A:', 'm']\n",
      "['l', 'a', 'N', 's', 'A:', 'm']\n",
      "batch: 15\n",
      "['k', 'r', 'y', '@', 'r', 'a', 'n', 'd', '@']\n",
      "['k', 'r', '9', 'x', '@', 'r', 'a', 'n', 'd', '@']\n",
      "['a', 'f', 'h_', 'a', 'N', 'k', 'l', '@', 'k', 'h_', '@i', 't']\n",
      "['a', 'f', 'h_', 'a', 'N', 'k', 'l', '@', 'k', 'h_', '@i', 't']\n",
      "['x', '@', 'l', 'i@', 'v', '@', 'r']\n",
      "['x', '@', 'l', 'i@', 'v', '@', 'r']\n",
      "['t', '9y', 'n', 'i', 'r']\n",
      "['t', '9y', 'n', 'i', 'r']\n",
      "['9y', 't', 'x', '@', 's', 't', '{', 'l']\n",
      "['9y', 't', 'x', '@', 's', 't', '{', 'l']\n",
      "batch: 16\n",
      "['k', '{', 'r', 'k', 'i@', 'n', 'h_', '@i', 't']\n",
      "['k', '{', 'r', 'k', 'i@', 'n', 'h_', '@i', 't']\n",
      "['f', '@', 'r', 't', '{', 'l', '@', 'r']\n",
      "['f', '@', 'r', 't', '{', 'l', '@', 'r']\n",
      "['s', 'l', '9y', 't', '@', 'N']\n",
      "['s', 'l', '9y', 't', '@', 'N']\n",
      "['k', 'l', '@', 'm', '@', 'r', 's']\n",
      "['k', 'l', '@', 'm', '@', 'r', 's']\n",
      "['k', 'r', '@', 'm', '@', 'n', 'i@', 'l']\n",
      "['k', 'r', '@', 'm', 'i', 'n', 'i@', 'l']\n",
      "batch: 17\n",
      "['p', 'r', 'u', 't', 'E', 'k', 't', 'u', 'r', 'A:', 't']\n",
      "['p', 'r', 'u', 't', 'E', 'k', 't', 'u', 'r', 'A:', 't']\n",
      "['l', 'i', 'd', '@', 'r', '@']\n",
      "['l', 'i', 'd', '@', 'r', '@']\n",
      "['s', 'k', '9', 't']\n",
      "['s', 'k', '9', 't']\n",
      "['t', 'r', 'a', 'x', 'i@', 'd', 'i', 's']\n",
      "['t', 'r', 'a', 'x', 'i@', 'd', 'i', 's']\n",
      "['s', '{', 'l', 'f', '@', 'r', 't', 'r', '@u', '@']\n",
      "['s', '{', 'l', 'f', '@', 'r', 't', 'r', '@u', '@']\n",
      "batch: 18\n",
      "['t', '{', 'l', '@', 'v', 'i', 's', 'i', 'p', 'r', 'u', 'x', 'r', 'a', 'm', '@']\n",
      "['t', '{', 'l', '@', 'v', 'i', 's', 'i', 'p', 'r', 'u', 'x', '@', 'm', 'a', 'r', '@']\n",
      "['i', 'l', 'E', 'k', 't', 'r', 'u', 'f', 'i', 'l', '@']\n",
      "['i', 'l', 'E', 'k', 't', 'r', 'u', 'f', 'i', 'l', '@']\n",
      "['a', 'r', 'x', '@', 't', 'i', 'p', 'i', 's', '@']\n",
      "['a', 'r', 'x', '@', 't', 'i', 'p', 'i', 's', '@']\n",
      "['h_', '9', 'l', 'p', '@', 'l', 'u@', 's']\n",
      "['h_', '9', 'l', 'p', '@', 'l', 'u@', 's']\n",
      "['s', 'E', 'n', 'd', '@', 'N', 'x', '@', 'n', 'u@', 't', 's', 'k', 'a', 'p']\n",
      "['s', 'E', 'n', 'd', '@', 'N', '@', 'N', 's', 't', 'u@', 'p', 'k', 'a', 'm']\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, eval_iter, cfg.device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
