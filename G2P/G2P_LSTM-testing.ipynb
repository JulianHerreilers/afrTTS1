{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Initialization of Config Class/File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "# from distance import levenshtein -> Not necessary as implemented from scratch\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "#from torchtext.data import BucketIterator\n",
    "\n",
    "seed = '4'\n",
    "if seed is not None:\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cfg class which is easliy translatible into a cfg file\n",
    "\n",
    "class Config:\n",
    "    seed = '5'\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dict_name = 'rcrl_apd.1.4.1.txt'\n",
    "    epochs = 200\n",
    "    batch_size = 128\n",
    "    hidden_dim = 256\n",
    "    embed_dim = 256\n",
    "    dropout = 0.1\n",
    "    dec_max_len = 30\n",
    "    MAX_LENGTH = 20\n",
    "    p_tf = 0.5\n",
    "    n_layers = 2\n",
    "    lr = 0.001\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(cfg.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of Datasets(using torchtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '</s>', \"'\", 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'á', 'ä', 'è', 'é', 'ê', 'ë', 'í', 'ï', 'ó', 'ô', 'ö', 'ú', 'û']\n",
      "['<pad>', '<s>', '</s>', '2:', '9', '9y', '@', '@i', '@u', 'A:', 'E', 'N', 'O', 'Of', 'S', 'Z', 'a', 'b', 'd', 'e', 'f', 'g', 'h_', 'i', 'i@', 'j', 'k', 'l', 'm', 'n', 'p', 'r', 's', 't', 'u', 'u@', 'v', 'w', 'x', 'y', 'z', '{']\n",
      "{'<pad>': 0, '<s>': 1, '</s>': 2, '2:': 3, '9': 4, '9y': 5, '@': 6, '@i': 7, '@u': 8, 'A:': 9, 'E': 10, 'N': 11, 'O': 12, 'Of': 13, 'S': 14, 'Z': 15, 'a': 16, 'b': 17, 'd': 18, 'e': 19, 'f': 20, 'g': 21, 'h_': 22, 'i': 23, 'i@': 24, 'j': 25, 'k': 26, 'l': 27, 'm': 28, 'n': 29, 'p': 30, 'r': 31, 's': 32, 't': 33, 'u': 34, 'u@': 35, 'v': 36, 'w': 37, 'x': 38, 'y': 39, 'z': 40, '{': 41}\n",
      "{'<pad>': 0, '</s>': 1, \"'\": 2, 'a': 3, 'b': 4, 'c': 5, 'd': 6, 'e': 7, 'f': 8, 'g': 9, 'h': 10, 'i': 11, 'j': 12, 'k': 13, 'l': 14, 'm': 15, 'n': 16, 'o': 17, 'p': 18, 'q': 19, 'r': 20, 's': 21, 't': 22, 'u': 23, 'v': 24, 'w': 25, 'x': 26, 'y': 27, 'z': 28, 'á': 29, 'ä': 30, 'è': 31, 'é': 32, 'ê': 33, 'ë': 34, 'í': 35, 'ï': 36, 'ó': 37, 'ô': 38, 'ö': 39, 'ú': 40, 'û': 41}\n"
     ]
    }
   ],
   "source": [
    "def dict_sorting (dict_file_name):\n",
    "    dict_file = open(dict_file_name, 'r')\n",
    "    lines_dict = dict_file.readlines()\n",
    "    dict_file.close()\n",
    "\n",
    "    graphemes = []\n",
    "    phonemes = []\n",
    "\n",
    "    for i in range(0, len(lines_dict)):\n",
    "        lines_dict[i] = lines_dict[i].split()\n",
    "        graphemes.append([*lines_dict[i][0]])\n",
    "        phonemes.append(lines_dict[i][1:])\n",
    "    phonemes = [a for b in phonemes for a in b]\n",
    "    graphemes = [a for b in graphemes for a in b]\n",
    "    graphemes = sorted(set(graphemes))\n",
    "    phonemes = sorted(set(phonemes))\n",
    "    return graphemes, phonemes\n",
    "\n",
    "dict_file_name = cfg.dict_name\n",
    "g_seq, p_seq = dict_sorting(dict_file_name)\n",
    "cfg.graphemes = [\"<pad>\", \"</s>\"] + g_seq\n",
    "cfg.phonemes = [\"<pad>\", \"<s>\", \"</s>\"] + p_seq\n",
    "\n",
    "#Index to grapheme and phones for vectors\n",
    "cfg.graph2index = {g: idx for idx, g in enumerate(cfg.graphemes)}\n",
    "cfg.index2graph = {idx: g for idx, g in enumerate(cfg.graphemes)}\n",
    "\n",
    "cfg.phone2index = {p: idx for idx, p in enumerate(cfg.phonemes)}\n",
    "cfg.index2phone = {idx: p for idx, p in enumerate(cfg.phonemes)}\n",
    "\n",
    "print(cfg.graphemes)\n",
    "print(cfg.phonemes)\n",
    "cfg.g_vocab_size = len(cfg.graphemes)\n",
    "cfg.p_vocab_size = len(cfg.phonemes)\n",
    "print(cfg.phone2index)\n",
    "print(cfg.graph2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataLoading(dict_file_name, data_split):\n",
    "    def sortingWP (d):\n",
    "        w, p = [], []\n",
    "        for i in range(0, len(d)):\n",
    "            #w.append(d[i][0])\n",
    "            w.append(' '.join(d[i][0]))\n",
    "            p.append((' '.join(d[i][1:])))\n",
    "        return w,p\n",
    "\n",
    "    with open(dict_file_name) as f:\n",
    "        dict_lines = f.readlines()\n",
    "    vocab_len = len(dict_lines)\n",
    "    print(vocab_len)\n",
    "    random.shuffle(dict_lines)\n",
    "\n",
    "    #Potential to add 'n to train dataset\n",
    "    \n",
    "    for i in range(0, len(dict_lines)):\n",
    "        dict_lines[i] = dict_lines[i].split()\n",
    "    train_data_lines, test_data_lines, eval_data_lines = [], [], []\n",
    "    train_data_lines = dict_lines[0:int(data_split[0]*vocab_len)]\n",
    "    test_data_lines = dict_lines[int(data_split[0]*vocab_len):int((data_split[0]+data_split[1])*vocab_len)]\n",
    "    eval_data_lines = dict_lines[int((data_split[0]+data_split[1])*vocab_len):]\n",
    "\n",
    "    train_word, train_phonemes = sortingWP(train_data_lines)\n",
    "    test_word, test_phonemes = sortingWP(test_data_lines)\n",
    "    eval_word, eval_phonemes = sortingWP(eval_data_lines)\n",
    "    \n",
    "    return train_word, train_phonemes, test_word, test_phonemes, eval_word, eval_phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def DataLoading(dict_file_name, data_split):\n",
    "#     def sortingWP (d):\n",
    "#         w, p = [], []\n",
    "#         for i in range(0, len(d)):\n",
    "#             #w.append(d[i][0])\n",
    "#             w.append(' '.join(d[i][0]))\n",
    "#             p.append((' '.join(d[i][1:])))\n",
    "#         return w,p\n",
    "\n",
    "#     with open(dict_file_name) as f:\n",
    "#         dict_lines = f.readlines()\n",
    "#     vocab_len = len(dict_lines)\n",
    "#     print(vocab_len)\n",
    "#     random.shuffle(dict_lines)\n",
    "\n",
    "#     #Potential to add 'n to train dataset\n",
    "    \n",
    "#     for i in range(0, len(dict_lines)):\n",
    "#         dict_lines[i] = dict_lines[i].split()\n",
    "#     train_data_lines, test_data_lines, eval_data_lines = [], [], []\n",
    "#     train_data_lines = dict_lines\n",
    "#     eval_data_lines = dict_lines[0:int(data_split[1]*vocab_len)]\n",
    "#     #test_data_lines = dict_lines[int(data_split[0]*vocab_len):int((data_split[0]+data_split[1])*vocab_len)]\n",
    "\n",
    "#     train_word, train_phonemes = sortingWP(train_data_lines)\n",
    "#     #test_word, test_phonemes = sortingWP(test_data_lines)\n",
    "#     eval_word, eval_phonemes = sortingWP(eval_data_lines)\n",
    "    \n",
    "#     return train_word, train_phonemes, eval_word, eval_phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split = [0.8, 0.1, 0.1]\n",
    "# train_word, train_phonemes = DataLoading(cfg.dict_name, split)\n",
    "\n",
    "# #Sanity Check\n",
    "# print(len(train_word))\n",
    "# print(len(train_phonemes))\n",
    "# print(train_word[0])\n",
    "# print(train_phonemes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24175\n",
      "19340\n",
      "2418\n"
     ]
    }
   ],
   "source": [
    "split = [0.8, 0.1, 0.1]\n",
    "train_word, train_phonemes, test_word, test_phonemes, eval_word, eval_phonemes = DataLoading(cfg.dict_name, split)\n",
    "#train_word, train_phonemes, eval_word, eval_phonemes = DataLoading(cfg.dict_name, split)\n",
    "#Sanity Check\n",
    "print(len(train_phonemes))\n",
    "print(len(eval_phonemes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Encoder & Decoder\n",
    "converts data to their dictionary equivalents based on indices(And decoder which will be used when finally checking sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_encoder(seq, isWord):\n",
    "    # Automatically encoders sequence with graph2index if words\n",
    "    tokenized_seq = []\n",
    "    if isWord: \n",
    "        seq = [*seq] + ['</s>']\n",
    "        seq = [i for i in seq if i!=\" \"]\n",
    "        for i in seq:\n",
    "            a = cfg.graph2index[i]\n",
    "            tokenized_seq.append(a)\n",
    "    #Else simply add end of sequence token to to phoneme sequences\n",
    "    else:\n",
    "        a = '<s> ' + str(seq) +' </s>'\n",
    "        seq = a.split(\" \")\n",
    "        ans = \"\"\n",
    "        for i in seq:\n",
    "            if i== 'o': i=\"O\"\n",
    "            elif i== 'h': i=\"h_\"\n",
    "            a = cfg.phone2index[i]\n",
    "            tokenized_seq.append(a)\n",
    "\n",
    "    #Tokenize sequence\n",
    "    return tokenized_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o n t s n a p p i n g\n",
      "O n t s n a p @ N\n",
      "[17, 16, 22, 21, 16, 3, 18, 18, 11, 16, 9, 1]\n",
      "[1, 12, 29, 33, 32, 29, 16, 30, 6, 11, 2]\n"
     ]
    }
   ],
   "source": [
    "print(train_word[0])\n",
    "print(train_phonemes[0])\n",
    "a = data_encoder(train_word[0], 1)\n",
    "b = data_encoder(train_phonemes[0], 0)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_decoder(sequence, isWord):\n",
    "    \"\"\"Converts index sequence back into corresponding letter tokens\"\"\"\n",
    "    if isWord: tokenizer = cfg.index2graph\n",
    "    else: tokenizer = cfg.index2phone\n",
    "    converted_sequence = []\n",
    "    \n",
    "    for i in sequence:\n",
    "        if tokenizer[i] == \"</s>\": break\n",
    "        a = tokenizer[i]\n",
    "        converted_sequence.append(a)\n",
    "    return converted_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['l', 'b', 'q', 'u', 'f', 'j', 'o', \"'\"]\n",
      "['l', 'b', 'r', 'u', '@u', 'p', '2:']\n"
     ]
    }
   ],
   "source": [
    "a = [14, 4, 19, 23, 8, 12, 17, 2]\n",
    "b = [27, 17, 31, 34, 8, 30, 3]\n",
    "a = data_decoder(a, 1)\n",
    "b = data_decoder(b, 0)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Class \n",
    "(Adapted from https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G2PData (data.Dataset):\n",
    "    def __init__(self, graphemes, phonemes):\n",
    "        self.graphemes = graphemes\n",
    "        self.phonemes = phonemes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphemes)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        graphemes = self.graphemes[index]\n",
    "        phonemes = self.phonemes[index]\n",
    "\n",
    "        #Fetches encoded versions\n",
    "        grapheme_vector = data_encoder(graphemes, 1)\n",
    "        phoneme_vector = data_encoder(phonemes, 0)\n",
    "\n",
    "        #Omits </s> character\n",
    "        decoder_inputs = phoneme_vector[:-1]\n",
    "        phoneme_vector = phoneme_vector[1:]\n",
    "\n",
    "        #Used for padding purposes\n",
    "        g_vec_len = len(grapheme_vector) \n",
    "        p_vec_len = len(phoneme_vector)\n",
    "        \n",
    "        return grapheme_vector, phoneme_vector, decoder_inputs, g_vec_len, p_vec_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2417\n"
     ]
    }
   ],
   "source": [
    "trainDataset = G2PData(train_word, train_phonemes)\n",
    "testDataset = G2PData(test_word, test_phonemes)\n",
    "print(len(test_word))\n",
    "evalDataset = G2PData(eval_word, eval_phonemes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding: To ensure datasets in the same batch are of the same length (Could also use bucketiterator to choose strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Setup\n",
    "Based on seq2seq tutorial for Machine Translation (https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb)\n",
    "\n",
    "Potential implementation of Attention: https://www.kaggle.com/code/omershect/learning-pytorch-seq2seq-with-m5-data-set/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, g_vocab_size, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embed = embed_dim\n",
    "        self.hidden = hidden_dim\n",
    "        self.embed = nn.Embedding(g_vocab_size, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, n_layers, batch_first = True)\n",
    "    \n",
    "    def forward(self, graph_seq, graph_seq_len):\n",
    "        embed_inputs = self.embed(graph_seq)\n",
    "        inputs = self.dropout(embed_inputs)\n",
    "\n",
    "        #https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html\n",
    "        #packs padded sequences into tensor\n",
    "        input_tensor = pack_padded_sequence(inputs, graph_seq_len, batch_first=True, enforce_sorted=False)\n",
    "        output, (hidden, context) = self.lstm(input_tensor)\n",
    "\n",
    "        return hidden, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, p_vocab_size, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embed = embed_dim\n",
    "        self.hidden = hidden_dim\n",
    "        self.embed = nn.Embedding(p_vocab_size, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, n_layers, batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_dim ,p_vocab_size) #Predicts output\n",
    "    \n",
    "    def forward(self, decoder_inputs, hidden_init, context_init):\n",
    "\n",
    "        \n",
    "        embed_inputs = self.embed(decoder_inputs)\n",
    "        inputs = self.dropout(embed_inputs)\n",
    "\n",
    "        #is already a tensor\n",
    "\n",
    "        output, (hidden, context) = self.lstm(inputs, (hidden_init, context_init))\n",
    "\n",
    "\n",
    "        #Transforming (batch_size,max_len,hidden_size) -> (batch_size,1,dictionary_size)\n",
    "\n",
    "        activation_output = self.fc(output)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        return activation_output, hidden, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G2PModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.enc = encoder\n",
    "        self.dec = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, graph_seq, graph_seq_len,phone_seq_len, decoder_inputs, phoneme_target_vec = None, training = False, teacher_forcing = cfg.p_tf):\n",
    "        \n",
    "        #Obtain hidden and context vectors from encoder\n",
    "        hidden_init, context_init = self.enc(graph_seq, graph_seq_len)\n",
    "        hidden, context = hidden_init, context_init\n",
    "\n",
    "        max_len = max(phone_seq_len)\n",
    "\n",
    "        phoneme_input_vec = decoder_inputs[:, :1]\n",
    "        outputs = [] \n",
    "        phone_pred_seq = []\n",
    "            \n",
    "        if training:\n",
    "            for i in range(0, max_len):\n",
    "\n",
    "                output, hidden, context = self.dec(phoneme_input_vec ,hidden, context)\n",
    "                outputs.append(output)\n",
    "                # phone_pred = torch.tensor(output.argmax(-1))\n",
    "\n",
    "                if random.random() > teacher_forcing: \n",
    "                    phoneme_input_vec = phoneme_target_vec[:,i]\n",
    "                    # print(\"teacher force\")\n",
    "                    # print(phoneme_input_vec.shape)\n",
    "                    \n",
    "                # else:  phoneme_input_vec = decoder_inputs[:,i]\n",
    "                else: \n",
    "                    phoneme_input_vec = torch.squeeze(output.argmax(-1))\n",
    "                    \n",
    "                    # print(\"not teacherforce\")\n",
    "                    # print(phoneme_input_vec.shape)\n",
    "                    # print(i)\n",
    "                    # print(phoneme_input_vec)\n",
    "                phoneme_input_vec = torch.unsqueeze(phoneme_input_vec, 1)\n",
    "\n",
    "        else: #for prediction\n",
    "            for i in range(1, cfg.dec_max_len+1):\n",
    "                output, hidden, context = self.dec(phoneme_input_vec ,hidden, context)\n",
    "                \n",
    "                phone_pred = output.argmax(-1)\n",
    "                outputs.append(output)\n",
    "                phone_pred_seq.append(phone_pred)\n",
    "                phoneme_input_vec = phone_pred\n",
    "                #print(i)\n",
    "                #print(phoneme_input_vec.shape)\n",
    "            phone_pred_seq = torch.cat(phone_pred_seq, 1)\n",
    "            \n",
    "        output = torch.cat(outputs, 1)\n",
    "        \n",
    "        return output, phone_pred_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterators Initialization and Padding\n",
    "\n",
    "Padding_data takes a batch and pads it for every iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_data(batch):\n",
    "\n",
    "    #Each sequence has a form:\n",
    "    # grapheme_vector, phoneme_vector, decoder_inputs, g_vec_len, p_vec_len, graphemes, phonemes\n",
    "\n",
    "    def get_components(batch, index):\n",
    "        ans = []\n",
    "        for i in batch:\n",
    "            ans.append(i[index])\n",
    "        return ans\n",
    "    \n",
    "    def pad_seq(batch, index, max_len):\n",
    "        ans = []\n",
    "        no_zeros_to_add = 0\n",
    "        for i in batch:\n",
    "            no_zeros_to_add = max_len - len(i[index])\n",
    "            ans.append(i[index] + [0] * no_zeros_to_add)\n",
    "        return torch.LongTensor(ans)\n",
    "    \n",
    "    grapheme_lens = [len(g[0]) for g in batch]\n",
    "\n",
    "    phonemes_lens = [len(p[1]) for p in batch]\n",
    "\n",
    "    input_maxlen = max(grapheme_lens)\n",
    "    output_maxlen = max(phonemes_lens)\n",
    "    padded_inputs = pad_seq(batch, 0, input_maxlen)\n",
    "    padded_outputs = pad_seq(batch, 1, output_maxlen)\n",
    "    padded_decoder_inputs = pad_seq(batch, 2, output_maxlen)\n",
    "\n",
    "    return padded_inputs, padded_outputs, padded_decoder_inputs, grapheme_lens, phonemes_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Loader Implementation\n",
    "#Shuffling not required as already loaded in a shuffled manner\n",
    "\n",
    "train_iter =  data.DataLoader(trainDataset,batch_size=cfg.batch_size, shuffle=False, collate_fn=padding_data)\n",
    "test_iter = data.DataLoader( testDataset,batch_size=cfg.batch_size, shuffle=False, collate_fn=padding_data)\n",
    "eval_iter = data.DataLoader(evalDataset,batch_size=cfg.batch_size, shuffle=False, collate_fn=padding_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model, Optimizer and Criterion Initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, device):\n",
    "    model.train() #sets model in training model\n",
    "    \n",
    "    loss_epoch = 0\n",
    "    \n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        grapheme_vector, phoneme_vector, decoder_inputs, g_vec_len, p_vec_len = batch\n",
    "\n",
    "        \n",
    "        #Placing vectors in GPU to streamline process\n",
    "        grapheme_vector = grapheme_vector.to(device)\n",
    "        phoneme_vector = phoneme_vector.to(device)\n",
    "        decoder_inputs = phoneme_vector.to(device)\n",
    "\n",
    "        optimizer.zero_grad() #Sets all gradients to zero\n",
    "\n",
    "\n",
    "        phoneme_pred,_ = model(grapheme_vector, g_vec_len,p_vec_len, decoder_inputs,phoneme_vector, True)\n",
    "\n",
    "        #phoneme_pred is in shape (batchsize, max_len, p_vocab_size) -> need to drop the last dimension\n",
    "\n",
    "        phoneme_pred = phoneme_pred.view(-1, phoneme_pred.shape[-1])\n",
    "        phoneme_vector = phoneme_vector.view(-1)\n",
    "\n",
    "        loss = criterion(phoneme_pred, phoneme_vector)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        #Normalizing Loss per batch\n",
    "        loss_epoch += loss.item()/len(batch)\n",
    "\n",
    "    return loss_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leven(target_word, pred_word):\n",
    "    lev_matrix = np.zeros((len(target_word)+1, len(pred_word)+1))\n",
    "    lev_matrix[0,:] = np.arange(0, len(pred_word) + 1)\n",
    "    lev_matrix[:,0] = np.arange(0, len(target_word) + 1)\n",
    "    for i in range(1, len(target_word)+1):\n",
    "        for j in range(1, len(pred_word)+1):\n",
    "            if target_word[i-1] == pred_word[j-1]:\n",
    "                lev_matrix[i,j] = lev_matrix[i-1,j-1]\n",
    "            else:\n",
    "                lev_matrix[i,j] = np.min([lev_matrix[i-1,j-1], lev_matrix[i,j-1], lev_matrix[i-1,j]]) + 1\n",
    "\n",
    "    return(lev_matrix[len(lev_matrix)-1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accuracy_PER_calculation(phoneme_vector_text, phoneme_predict_text):\n",
    "    \"Accuracy - Words that are not 100% correct\"\n",
    "    \"PER - Rate at which phonemes are incorrectly generated - Levenshtein distance/n_phones in correct text\"\n",
    "    n_wrong_words =   0\n",
    "    total_phones_real = 0\n",
    "    n_incorrect_phones = 0\n",
    "\n",
    "    n_entries = len(phoneme_vector_text)\n",
    "\n",
    "    for i in range(n_entries):\n",
    "        total_phones_real += len(phoneme_vector_text[i])\n",
    "        ans = leven(phoneme_vector_text[i], phoneme_predict_text[i])\n",
    "        n_incorrect_phones += ans\n",
    "        if ans !=0 : n_wrong_words +=1\n",
    "        \n",
    "    #phoneme_error_rate = incorrect_phones/total_phones_real - incorrect as doesnt allow a sensible way to work out PER over entire epoch\n",
    "    #accuracy = n_wrong_words/n_entries\n",
    "    \n",
    "    return n_incorrect_phones, total_phones_real, n_wrong_words, n_entries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, device):\n",
    "\n",
    "    def pad_p_vec(phoneme_vector, max_len):\n",
    "        phoneme_vector = phoneme_vector.cpu().numpy()\n",
    "        \n",
    "        ans = []\n",
    "\n",
    "        for i in phoneme_vector:\n",
    "            no_zeros_to_add = (max_len - len(i))\n",
    "            j = np.array(no_zeros_to_add * [0])\n",
    "            j = j.reshape((no_zeros_to_add, 1))\n",
    "            i = i.reshape((len(i), 1))\n",
    "\n",
    "            ans.append(np.concatenate((i, j)))\n",
    "        #print(ans.sha)\n",
    "        ans = np.asarray(ans)\n",
    "            \n",
    "        return torch.LongTensor(ans)\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    loss_epoch = 0\n",
    "    n_incorrect_phones, total_phones_real, n_wrong_words, n_entries = 0, 0, 0, 0\n",
    "    val_real, val_pred = [],[]\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, batch in enumerate(iterator):\n",
    "            \n",
    "            grapheme_vector, phoneme_vector, decoder_inputs, g_vec_len, p_vec_len = batch\n",
    "\n",
    "            grapheme_vector = grapheme_vector.to(device)\n",
    "            phoneme_vector = phoneme_vector.to(device)\n",
    "            decoder_inputs = phoneme_vector.to(device)\n",
    "\n",
    "\n",
    "            #graph_seq, graph_seq_len, decoder_inputs, training = False, teacher_forcing = cfg.p_tf\n",
    "\n",
    "            phoneme_pred, phoneme_pred_sequence = model(grapheme_vector, g_vec_len,p_vec_len, decoder_inputs, phoneme_vector, False) #False means only prediction and no training & teacher-forcing\n",
    "            #print(phoneme_pred.shape)\n",
    "            #print(f\"before:{phoneme_vector.shape}\")\n",
    "\n",
    "            phoneme_vector = pad_p_vec(phoneme_vector, cfg.dec_max_len)\n",
    "            phoneme_vector_target = phoneme_vector\n",
    "\n",
    "            phoneme_vector_target = phoneme_vector_target.to(device)\n",
    "            phoneme_pred = phoneme_pred.view(-1, phoneme_pred.shape[-1])\n",
    "            phoneme_vector_target = phoneme_vector_target.view(-1)\n",
    "\n",
    "            loss = criterion(phoneme_pred, phoneme_vector_target)\n",
    "\n",
    "\n",
    "\n",
    "            # First move tensor to CPU then to numpy array for decoding source: https://stackoverflow.com/questions/49768306/pytorch-tensor-to-numpy-array\n",
    "\n",
    "            # print(f\"batch: {i}\")\n",
    "            # # for j in range(5):\n",
    "            text_real, text_pred = [], []\n",
    "            \n",
    "            phoneme_vector = torch.squeeze(phoneme_vector)\n",
    "            for j in range(len(batch)):\n",
    "                text_real.append(data_decoder(phoneme_vector[j].cpu().numpy().tolist(), 0))\n",
    "                text_pred.append(data_decoder(phoneme_pred_sequence[j].cpu().numpy().tolist(), 0))\n",
    "\n",
    "              \n",
    "            a, b, c, d = Accuracy_PER_calculation(text_pred, text_real)\n",
    "\n",
    "            #n_incorrect_phones, total_phones_real, n_wrong_words, n_entries\n",
    "            n_incorrect_phones += a\n",
    "            total_phones_real += b\n",
    "            n_wrong_words += c\n",
    "            n_entries += d\n",
    "            #print(f\"a: {a} b: {b} c: {c} d: {d}\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "            loss_epoch += loss.item()/len(batch)\n",
    "        val_real.append(text_real[0])\n",
    "        val_pred.append(text_pred[0])\n",
    "    per_accuracy = 1.0 -(n_wrong_words / n_entries)\n",
    "    phoneme_error_rate = n_incorrect_phones / total_phones_real\n",
    "\n",
    "\n",
    "    return loss_epoch, per_accuracy, phoneme_error_rate, n_wrong_words, val_real, val_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(hidden_size, embed_size, n_layers, epochs):\n",
    "\n",
    "    combined_best_epochs = []\n",
    "    model_name = f\"G2P-e{embed_size}h{hidden_size}n{n_layers}d{cfg.dropout}\"\n",
    "    encoder = Encoder(embed_size, hidden_size, cfg.g_vocab_size, n_layers, cfg.dropout)\n",
    "    decoder = Decoder(embed_size, hidden_size, cfg.g_vocab_size, n_layers, cfg.dropout)\n",
    "\n",
    "    model = G2PModel(encoder, decoder, cfg.device)\n",
    "    #print(torch.cuda.is_available())\n",
    "    model.to(device=cfg.device)\n",
    "\n",
    "\n",
    "    def count_params(model):\n",
    "        return sum(a.numel() for a in model.parameters() if a.requires_grad)\n",
    "\n",
    "    #print(f'The model has {count_params(model):,} trainable parameters')\n",
    "    optimizer = optim.Adam(model.parameters(), lr=cfg.lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0) #Ignores index corresponding to padding\n",
    "\n",
    "    prev_best_loss = math.inf\n",
    "\n",
    "\n",
    "    print(\"Training\")\n",
    "    matrix_exists = 0\n",
    "    max_bad_losses = 6\n",
    "    loss_reset = 0\n",
    "    val_real_phonemes_best, val_pred_phonemes_best = [], []\n",
    "\n",
    "    for i in range(0, epochs):\n",
    "\n",
    "        train_loss = train(model, train_iter, optimizer, criterion, cfg.device)\n",
    "        \n",
    "        eval_loss, acc, phoneme_error_rate, n_wrong_words, val_real,val_pred = evaluate(model, eval_iter, criterion, cfg.device)\n",
    "\n",
    "        \n",
    "        if matrix_exists == 0:\n",
    "            combined_losses = np.array((i, train_loss, eval_loss, acc, phoneme_error_rate, n_wrong_words))\n",
    "            matrix_exists = 1\n",
    "        else:\n",
    "            combined_losses= np.vstack([combined_losses,(i, train_loss, eval_loss, acc, phoneme_error_rate, n_wrong_words)])\n",
    "\n",
    "        #print(f\"train loss: {round(train_loss, 3)}    eval loss: {round(eval_loss, 3)}    Accuracy: {round(acc, 3)}   PER: {round(phoneme_error_rate, 3)}    Incorrect words: {round(n_wrong_words, 3)}\")\n",
    "\n",
    "        if prev_best_loss > eval_loss:\n",
    "            prev_best_loss = eval_loss\n",
    "            loss_reset = 0\n",
    "            best_epoch = i\n",
    "            val_pred_phonemes_best = val_pred\n",
    "            val_real_phonemes_best = val_real\n",
    "        #     if i > 5:\n",
    "            #torch.save(model.state_dict(), \"test_models/\"+model_name+\".pt\")\n",
    "    \n",
    "        # else: loss_reset +=1\n",
    "\n",
    "        #if loss_reset == max_bad_losses: break\n",
    "\n",
    "    \n",
    "\n",
    "    return model_name,best_epoch, combined_losses, val_real_phonemes_best, val_pred_phonemes_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(cfg.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "True\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6787/55672745.py:35: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  temp = np.array(temp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "True\n",
      "2\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@']]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/junix/afrTTS/G2P/G2P_LSTM-testing.ipynb Cell 35\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM-testing.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m#loss_epoch, per_accuracy, phoneme_error_rate, n_wrong_words, val_real, val_pred\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM-testing.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,epochs):\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM-testing.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m     loss_epoch, per_accuracy, phoneme_error_rate, n_wrong_words, val_real, val_pred\u001b[39m=\u001b[39m evaluate(model, test_iter, criterion, cfg\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM-testing.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39mprint\u001b[39m(val_real)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM-testing.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     temp\u001b[39m.\u001b[39mappend((loss_epoch, \u001b[39m100\u001b[39m\u001b[39m*\u001b[39mper_accuracy, \u001b[39m100\u001b[39m\u001b[39m*\u001b[39mphoneme_error_rate, n_wrong_words, val_real, val_pred))\n",
      "\u001b[1;32m/home/junix/afrTTS/G2P/G2P_LSTM-testing.ipynb Cell 35\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, iterator, criterion, device)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM-testing.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m decoder_inputs \u001b[39m=\u001b[39m phoneme_vector\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM-testing.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m#graph_seq, graph_seq_len, decoder_inputs, training = False, teacher_forcing = cfg.p_tf\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM-testing.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m phoneme_pred, phoneme_pred_sequence \u001b[39m=\u001b[39m model(grapheme_vector, g_vec_len,p_vec_len, decoder_inputs, phoneme_vector, \u001b[39mFalse\u001b[39;49;00m) \u001b[39m#False means only prediction and no training & teacher-forcing\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM-testing.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m#print(phoneme_pred.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM-testing.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m#print(f\"before:{phoneme_vector.shape}\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM-testing.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m phoneme_vector \u001b[39m=\u001b[39m pad_p_vec(phoneme_vector, cfg\u001b[39m.\u001b[39mdec_max_len)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/junix/afrTTS/G2P/G2P_LSTM-testing.ipynb Cell 35\u001b[0m in \u001b[0;36mG2PModel.forward\u001b[0;34m(self, graph_seq, graph_seq_len, phone_seq_len, decoder_inputs, phoneme_target_vec, training, teacher_forcing)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM-testing.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39melse\u001b[39;00m: \u001b[39m#for prediction\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM-testing.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, cfg\u001b[39m.\u001b[39mdec_max_len\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM-testing.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m         output, hidden, context \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdec(phoneme_input_vec ,hidden, context)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM-testing.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m         phone_pred \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39margmax(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM-testing.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m         outputs\u001b[39m.\u001b[39mappend(output)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/junix/afrTTS/G2P/G2P_LSTM-testing.ipynb Cell 35\u001b[0m in \u001b[0;36mDecoder.forward\u001b[0;34m(self, decoder_inputs, hidden_init, context_init)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM-testing.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(embed_inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM-testing.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m#is already a tensor\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM-testing.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m output, (hidden, context) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(inputs, (hidden_init, context_init))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM-testing.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m#Transforming (batch_size,max_len,hidden_size) -> (batch_size,1,dictionary_size)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junix/afrTTS/G2P/G2P_LSTM-testing.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m activation_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(output)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:769\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    768\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    770\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    771\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    773\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from numpy import float128\n",
    "\n",
    "\n",
    "epochs = 30\n",
    "models_to_test = [\"test_models/G2p-e64h64n2d0.1.pt\",\"test_models/G2p-e128h128n2d0.1.pt\",\"test_models/G2p-e256h256n2d0.1.pt\",\"test_models/G2P-e512h512n2d0.1.pt\"\n",
    "                , \"test_models/G2P-e128h256n2d0.1.pt\",\"test_models/G2P-e256h128n2d0.1.pt\",\"test_models/G2p-e128h128n1d0.1.pt\",\"test_models/G2p-e128h128n3d0.1.pt\"]\n",
    "final = []\n",
    "\n",
    "for l,i in enumerate(models_to_test):\n",
    "\n",
    "    model_name = i.replace(\"test_models/G2p-\",\"\").replace(\"test_models/G2P-\",\"\")\n",
    "    model_name = i.replace(\"test_models/G2p-\",\"\").replace(\"test_models/G2P-\",\"\")\n",
    "\n",
    "    e = int(model_name[model_name.index(\"e\")+1:model_name.index(\"h\")])\n",
    "    h = int(model_name[model_name.index(\"h\")+1:model_name.index(\"n\")])\n",
    "    n = int(model_name[model_name.index(\"n\")+1:model_name.index(\"d\")])\n",
    "    d = float(model_name[model_name.index(\"d\")+1:model_name.index(\"d\")+4])\n",
    "\n",
    "    encoder = Encoder(e, h, cfg.g_vocab_size, n, d)\n",
    "    decoder = Decoder(e, h, cfg.g_vocab_size, n, d)\n",
    "\n",
    "    model = G2PModel(encoder, decoder, cfg.device)\n",
    "    model.to(cfg.device)\n",
    "    model.load_state_dict(torch.load(i))\n",
    "    print(next(model.parameters()).is_cuda)\n",
    "    \n",
    "    temp = []\n",
    "    print(l)\n",
    "    #loss_epoch, per_accuracy, phoneme_error_rate, n_wrong_words, val_real, val_pred\n",
    "    for j in range(0,epochs):\n",
    "        loss_epoch, per_accuracy, phoneme_error_rate, n_wrong_words, val_real, val_pred= evaluate(model, test_iter, criterion, cfg.device)\n",
    "        print(val_real)\n",
    "        temp.append((loss_epoch, 100*per_accuracy, 100*phoneme_error_rate, n_wrong_words, val_real, val_pred))\n",
    "\n",
    "    temp = np.array(temp)\n",
    "\n",
    "    final.append((model_name, sum(a.numel() for a in model.parameters() if a.requires_grad) ,np.average(temp[:,1]), float128(np.average(np.abs(np.average(temp[:,1])-temp[:,1]))), np.average(temp[:,2]), np.std(temp[:,2]), temp[-2][-2], temp[-2][-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@'], ['b', '@', 'x', 'r', 'a', 'f', 'n', '@', 's'], ['b', '@', 's', 'E', 'n', 'd', '@', 'N', 's'], ['p', 'u', 'z', 'i', 'S', 'i', 'u', 'n', 'i@', 'r', '@', 'N'], ['f', '@', 'r', 't', 'u', 'f']]]\n",
      "[[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@'], ['b', '@', 'x', 'r', 'a', 'f', 'n', '@', 's'], ['b', '@', 's', 'E', 'n', 'd', '@', 'N', 's'], ['p', 'u', 's', 'i', 'S', 'i', 'u', 'n', 'i@', 'r', '@', 'N'], ['f', '@', 'r', 't', 'u', 'f']]]\n"
     ]
    }
   ],
   "source": [
    "print(temp[10][-2])\n",
    "print(temp[-3][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@'],\n",
       "  ['b', '@', 'x', 'r', 'a', 'f', 'n', '@', 's'],\n",
       "  ['b', '@', 's', 'E', 'n', 'd', '@', 'N', 's'],\n",
       "  ['p', 'u', 's', 'i', 'S', 'i', 'u', 'n', 'i@', 'r', '@', 'N'],\n",
       "  ['f', '@', 'r', 't', 'u', 'f']]]"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[-1,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@'], ['b', '@', 'x', 'r', 'a', 'f', 'n', '@', 's'], ['b', '@', 's', 'E', 'n', 'd', '@', 'N', 's'], ['p', 'u', 'z', 'i', 'S', 'i', 'u', 'n', 'i@', 'r', '@', 'N'], ['f', '@', 'r', 't', 'u', 'f']]\n",
      "('e64h64n2d0.1.pt', 141226, 77.89473684210525, 1.4210854715202003718e-14, 7.407407407407411, 4.440892098500626e-15)\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@'], ['b', '@', 'x', 'r', 'A:', 'f', 'n', 's', '@'], ['b', '@', 's', 'E', 'n', 'd', '@', 'N', 's'], ['p', 'u@', 's', 'i', 'S', 'i', 'u', 'n', 'i@', 'r', '@', 'N'], ['f', '@', 'r', 't', 'u', 'f']]\n",
      "('e128h128n2d0.1.pt', 544554, 75.78947368421058, 4.2632564145606011152e-14, 5.6241426611796985, 0.0)\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@'], ['b', '@', 'x', 'r', 'a', 'f', 'n', '@', 's'], ['b', '@', 's', 'E', 'n', 'd', '@', 'N', 's'], ['p', 'u@', 'S', 'i', 'u', 'S', 'i', 'n', 'i@', 'r', '@', 'N'], ['f', '@', 'r', 't', 'u', 'f']]\n",
      "('e256h256n2d0.1.pt', 2137642, 87.36842105263163, 4.2632564145606011152e-14, 2.8885832187070135, 1.7763568394002505e-15)\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@'], ['b', '@', 'x', 'r', 'a', 'f', 'n', '@', 's'], ['b', '@', 's', 'E', 'n', 'd', '@', 'N', 's'], ['p', 'u', 's', 'i', 'S', 'i', 'u', 'n', 'i@', 'r', '@', 'N'], ['f', '@', 'r', 't', 'u', 'f']]\n",
      "('e512h512n2d0.1.pt', 8469546, 82.10526315789474, 0.0, 3.9726027397260277, 0.0)\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@'], ['b', '@', 'x', 'r', 'a', 'f', 'n', '@', 's'], ['b', '@', 's', 'E', 'n', 'd', '@', 'N', 's'], ['p', 'u', 'z', 'i', 'S', 'i', 'u', 'n', 'i@', 'r', '@', 'N'], ['f', '@', 'r', 't', 'u', 'f']]\n",
      "('e128h256n2d0.1.pt', 1864746, 85.26315789473682, 1.4210854715202003718e-14, 4.223433242506814, 1.7763568394002505e-15)\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@'], ['b', '@', 'x', 'r', 'a', 'f', 'n', '@', 's'], ['b', '@', 's', 'E', 'n', 'd', '@', 'N', 's'], ['p', 'u', 'z', 'i', 'S', 'i', 'u', 'n', 'i@', 'r', '@', 'N'], ['f', '@', 'r', 't', 'u', 'f']]\n",
      "('e256h128n2d0.1.pt', 686378, 83.1578947368421, 1.4210854715202003718e-14, 5.075445816186556, 8.881784197001252e-16)\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@'], ['b', '@', 'x', 'r', 'a', 'f', 'n', '@', 's'], ['b', '@', 's', 'E', 'n', 'd', '@', 'N', 's'], ['p', 'u', 'z', 'i', 'S', 'i', 'u', 'n', 'i@', 'r', '@', 'N'], ['f', '@', 'r', 't', 'u', 'f']]\n",
      "('e128h128n1d0.1.pt', 280362, 73.68421052631582, 2.8421709430404007435e-14, 8.665749656121049, 3.552713678800501e-15)\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@'], ['b', '@', 'x', 'r', 'a', 'f', 'n', '@', 's'], ['b', '@', 's', 'E', 'n', 'd', '@', 'N', 's'], ['p', 'u@', 's', '@', 'S', 'i', 'u', 'n', 'i@', 'r', '@', 'N'], ['f', '@', 'r', 't', 'u', 'f']]\n",
      "('e128h128n3d0.1.pt', 808746, 80.0, 0.0, 5.486968449931414, 8.881784197001252e-16)\n",
      "[['O', 'p', 't', 'i', 'm', 'A:', 'l', '@'], ['b', '@', 'x', 'r', 'a', 'f', 'n', '@', 's'], ['b', '@', 's', 'E', 'n', 'd', '@', 'N', 's'], ['p', 'u', 's', 'i', 'S', 'i', 'u', 'n', 'i@', 'r', '@', 'N'], ['f', '@', 'r', 't', 'u', 'f']]\n"
     ]
    }
   ],
   "source": [
    "index = -1\n",
    "for line in final:\n",
    "    if final.index(line)==0:\n",
    "        print(line[-2][index])\n",
    "    print(line[0:-2])\n",
    "    print(line[-1][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot assign to literal (548767052.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [382]\u001b[0;36m\u001b[0m\n\u001b[0;31m    1=2\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m cannot assign to literal\n"
     ]
    }
   ],
   "source": [
    "1=2\n",
    "for line in final:\n",
    "    print(line[0:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3431\n"
     ]
    }
   ],
   "source": [
    "def DataLoading_forG2P(data_file):\n",
    "    g2p_start = {\"a\": \"a\", \"b\": \"b\", \"c\":\"k\", \"d\":\"d\", \"e\":\"E\", \"f\":\"f\", \"g\":\"x\", \"h\":\"h_\", \"i\":\"i\",\n",
    "    \"j\": \"j\", \"k\":\"k\", \"l\":\"l\", \"m\":\"m\", \"n\":\"n\", \"o\":\"O\", \"p\":\"p\", \"q\":\"k\", \"r\":\"r\", \"s\":\"s\",\n",
    "    \"t\":\"t\", \"u\":\"9y\", \"v\":\"f\", \"w\":\"v\", \"x\":\"z\", \"y\":\"@i\", \"z\":\"z\", \"\\'\":\"A:\" }\n",
    "    def sortingWP (d):\n",
    "        w, p = [], []\n",
    "        for i in range(0, len(d)):\n",
    "\n",
    "            #w.append(d[i][0])\n",
    "            \n",
    "            #w.append(' '.join(j for j in d[i]))\n",
    "            w.append(\" \".join(d[i]))\n",
    "            #p.append(\"2:\") \n",
    "            if d[i][0] ==\"a\" and d[i][1] ==\"a\": p.append(\"A:\")\n",
    "            else: p.append(g2p_start[d[i][0]]) # #Starting phone based on starting graph\n",
    "        return w,p\n",
    "\n",
    "    with open(data_words_to_G2P) as f:\n",
    "        data_lines = f.readlines()\n",
    "    vocab_len = len(data_lines)\n",
    "    print(vocab_len)\n",
    "\n",
    "    #Potential to add 'n to train dataset\n",
    "    \n",
    "    for i in range(0, len(data_lines)):\n",
    "        data_lines[i] = data_lines[i].replace(\"\\n\",\"\")\n",
    "    train_data_lines = []\n",
    "    train_data_lines = data_lines\n",
    "    train_word, train_phonemes = sortingWP(data_lines)\n",
    "    \n",
    "    return train_word, train_phonemes\n",
    "\n",
    "data_words_to_G2P = \"words_to_G2P.txt\"\n",
    "data_set_G, data_set_P = DataLoading_forG2P(data_words_to_G2P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<s>', '</s>', '2:', '9', '9y', '@', '@i', '@u', 'A:', 'E', 'N', 'O', 'Of', 'S', 'Z', 'a', 'b', 'd', 'e', 'f', 'g', 'h_', 'i', 'i@', 'j', 'k', 'l', 'm', 'n', 'p', 'r', 's', 't', 'u', 'u@', 'v', 'w', 'x', 'y', 'z', '{']\n"
     ]
    }
   ],
   "source": [
    "print(cfg.phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def init_model():\n",
    "    encoder = Encoder(cfg.embed_dim, cfg.hidden_dim, cfg.g_vocab_size, cfg.n_layers, cfg.dropout)\n",
    "    decoder = Decoder(cfg.embed_dim, cfg.hidden_dim, cfg.g_vocab_size, cfg.n_layers, cfg.dropout)\n",
    "\n",
    "    model = G2PModel(encoder, decoder, cfg.device)\n",
    "    model_params = \"best_models/G2pLSTM-e64h128n2.pt\"\n",
    "    model_paramsb = \"G2pLSTM-e64h128n2train.pt\"\n",
    "    #model_paramsc = \"models/G2pLSTM-e256h256n2trainwitheval.pt\"\n",
    "    #model.load_state_dict(torch.load(model_paramsc))\n",
    "    print(torch.cuda.is_available())\n",
    "\n",
    "startnload_new_model = 1\n",
    "if startnload_new_model: init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3431\n"
     ]
    }
   ],
   "source": [
    "data_words_to_G2P = \"words_to_G2P.txt\"\n",
    "pd_file = \"pd_for_data.txt\"\n",
    "data_set_G, data_set_P = DataLoading_forG2P(data_words_to_G2P)\n",
    "dataset_to_g2p = G2PData(data_set_G, data_set_P)\n",
    "dataset_to_g2p_iter =  data.DataLoader(dataset_to_g2p,batch_size=cfg.batch_size, shuffle=False, collate_fn=padding_data)\n",
    "generate_pd(model, dataset_to_g2p_iter, pd_file, cfg.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created with assistance from tutorials and adapted from Sources:\n",
    "1. https://github.com/Kyubyong/g2p - used for base notebook layout and initial introduction into structure of a G2P model - had no mention of teacherforcing\n",
    "https://github.com/Kyubyong/nlp_made_easy/blob/master/PyTorch%20seq2seq%20template%20based%20on%20the%20g2p%20task.ipynb\n",
    "2. https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e#58f2 - assistance in understanding various concepts and functions in [1]\n",
    "2. https://github.com/bentrevett/pytorch-seq2seq\n",
    "3. https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "\n",
    "4. Further assistance on DataLoader and BucketIterator from: https://colab.research.google.com/github/gmihaila/ml_things/blob/master/notebooks/pytorch/pytorchtext_bucketiterator.ipynb#scrollTo=ChQWVc4IUUPb\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
